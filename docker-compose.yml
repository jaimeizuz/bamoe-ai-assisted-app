services:
  nginx:
    hostname: ${NGINX_SERVICE_HOST}
    image: nginx:${NGINX_VERSION}
    deploy:
      resources:
        limits:
          cpus: ${NGINX_CPUS}
          memory: ${NGINX_MEM}
        reservations:
          memory: ${NGINX_MEM}
    profiles:
      - nginx
    volumes:
    - ./nginx/templates:/etc/nginx/templates:Z
    ports:
      - ${NGINX_SERVICE_PORT}:${NGINX_INTERNAL_PORT}
    environment:
    - NGINX_HOST=${NGINX_INTERNAL_HOST}
    - NGINX_PORT=${NGINX_INTERNAL_PORT}
    depends_on:
      langflow:
        condition: service_started
  langfuse-worker:
    hostname: ${LANGFUSE_WORKER_SERVICE_HOST}
    image: docker.io/langfuse/langfuse-worker:${LANGFUSE_VERSION}
    restart: always
    depends_on: &langfuse-depends-on
      postgres-server:
        condition: service_healthy
      minio:
        condition: service_healthy
      redis:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    ports:
      - 127.0.0.1:${LANGFUSE_WORKER_SERVICE_PORT}:${LANGFUSE_WORKER_INTERNAL_PORT}
    environment: &langfuse-worker-env
      HOSTNAME: ${LANGFUSE_WORKER_INTERNAL_HOST}
      NEXTAUTH_URL: ${NEXTAUTH_URL:-http://${HOST_LOCAL_IP}:${LANGFUSE_WEB_SERVICE_PORT}}
      DATABASE_URL: ${DATABASE_URL:-postgresql://postgres:postgres@${POSTGRESQL_SERVICE_HOST}:${POSTGRESQL_INTERNAL_PORT}/postgres} # CHANGEME
      SALT: ${SALT:-mysalt} # CHANGEME
      ENCRYPTION_KEY: ${ENCRYPTION_KEY:-0000000000000000000000000000000000000000000000000000000000000000} # CHANGEME: generate via `openssl rand -hex 32`
      TELEMETRY_ENABLED: ${TELEMETRY_ENABLED:-true}
      LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: ${LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES:-true}
      CLICKHOUSE_MIGRATION_URL: ${CLICKHOUSE_MIGRATION_URL:-clickhouse://clickhouse:9000}
      CLICKHOUSE_URL: ${CLICKHOUSE_URL:-http://clickhouse:8123}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-clickhouse}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-clickhouse} # CHANGEME
      CLICKHOUSE_CLUSTER_ENABLED: ${CLICKHOUSE_CLUSTER_ENABLED:-false}
      LANGFUSE_USE_AZURE_BLOB: ${LANGFUSE_USE_AZURE_BLOB:-false}
      LANGFUSE_S3_EVENT_UPLOAD_BUCKET: ${LANGFUSE_S3_EVENT_UPLOAD_BUCKET:-langfuse}
      LANGFUSE_S3_EVENT_UPLOAD_REGION: ${LANGFUSE_S3_EVENT_UPLOAD_REGION:-auto}
      LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID: ${LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID:-minio}
      LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY: ${LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY:-miniosecret} # CHANGEME
      LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT: ${LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT:-http://minio:9000}
      LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE: ${LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE:-true}
      LANGFUSE_S3_EVENT_UPLOAD_PREFIX: ${LANGFUSE_S3_EVENT_UPLOAD_PREFIX:-events/}
      LANGFUSE_S3_MEDIA_UPLOAD_BUCKET: ${LANGFUSE_S3_MEDIA_UPLOAD_BUCKET:-langfuse}
      LANGFUSE_S3_MEDIA_UPLOAD_REGION: ${LANGFUSE_S3_MEDIA_UPLOAD_REGION:-auto}
      LANGFUSE_S3_MEDIA_UPLOAD_ACCESS_KEY_ID: ${LANGFUSE_S3_MEDIA_UPLOAD_ACCESS_KEY_ID:-minio}
      LANGFUSE_S3_MEDIA_UPLOAD_SECRET_ACCESS_KEY: ${LANGFUSE_S3_MEDIA_UPLOAD_SECRET_ACCESS_KEY:-miniosecret} # CHANGEME
      LANGFUSE_S3_MEDIA_UPLOAD_ENDPOINT: ${LANGFUSE_S3_MEDIA_UPLOAD_ENDPOINT:-http://localhost:9090}
      LANGFUSE_S3_MEDIA_UPLOAD_FORCE_PATH_STYLE: ${LANGFUSE_S3_MEDIA_UPLOAD_FORCE_PATH_STYLE:-true}
      LANGFUSE_S3_MEDIA_UPLOAD_PREFIX: ${LANGFUSE_S3_MEDIA_UPLOAD_PREFIX:-media/}
      LANGFUSE_S3_BATCH_EXPORT_ENABLED: ${LANGFUSE_S3_BATCH_EXPORT_ENABLED:-false}
      LANGFUSE_S3_BATCH_EXPORT_BUCKET: ${LANGFUSE_S3_BATCH_EXPORT_BUCKET:-langfuse}
      LANGFUSE_S3_BATCH_EXPORT_PREFIX: ${LANGFUSE_S3_BATCH_EXPORT_PREFIX:-exports/}
      LANGFUSE_S3_BATCH_EXPORT_REGION: ${LANGFUSE_S3_BATCH_EXPORT_REGION:-auto}
      LANGFUSE_S3_BATCH_EXPORT_ENDPOINT: ${LANGFUSE_S3_BATCH_EXPORT_ENDPOINT:-http://minio:9000}
      LANGFUSE_S3_BATCH_EXPORT_EXTERNAL_ENDPOINT: ${LANGFUSE_S3_BATCH_EXPORT_EXTERNAL_ENDPOINT:-http://localhost:9090}
      LANGFUSE_S3_BATCH_EXPORT_ACCESS_KEY_ID: ${LANGFUSE_S3_BATCH_EXPORT_ACCESS_KEY_ID:-minio}
      LANGFUSE_S3_BATCH_EXPORT_SECRET_ACCESS_KEY: ${LANGFUSE_S3_BATCH_EXPORT_SECRET_ACCESS_KEY:-miniosecret} # CHANGEME
      LANGFUSE_S3_BATCH_EXPORT_FORCE_PATH_STYLE: ${LANGFUSE_S3_BATCH_EXPORT_FORCE_PATH_STYLE:-true}
      LANGFUSE_INGESTION_QUEUE_DELAY_MS: ${LANGFUSE_INGESTION_QUEUE_DELAY_MS:-}
      LANGFUSE_INGESTION_CLICKHOUSE_WRITE_INTERVAL_MS: ${LANGFUSE_INGESTION_CLICKHOUSE_WRITE_INTERVAL_MS:-}
      REDIS_HOST: ${REDIS_HOST}
      REDIS_PORT: ${REDIS_PORT}
      REDIS_AUTH: ${REDIS_AUTH}
      REDIS_TLS_ENABLED: ${REDIS_TLS_ENABLED:-false}
      REDIS_TLS_CA: ${REDIS_TLS_CA:-/certs/ca.crt}
      REDIS_TLS_CERT: ${REDIS_TLS_CERT:-/certs/redis.crt}
      REDIS_TLS_KEY: ${REDIS_TLS_KEY:-/certs/redis.key}
      EMAIL_FROM_ADDRESS: ${EMAIL_FROM_ADDRESS:-}
      SMTP_CONNECTION_URL: ${SMTP_CONNECTION_URL:-}
  langfuse-web:
    hostname: ${LANGFUSE_WEB_SERVICE_HOST}
    image: docker.io/langfuse/langfuse:${LANGFUSE_VERSION}
    restart: always
    depends_on: *langfuse-depends-on
    ports:
      - ${LANGFUSE_WEB_SERVICE_PORT}:${LANGFUSE_WEB_INTERNAL_PORT}
    environment:
      <<: *langfuse-worker-env
      NEXTAUTH_SECRET: ${NEXTAUTH_SECRET:-mysecret} # CHANGEME
      HOSTNAME: ${LANGFUSE_WEB_INTERNAL_HOST}
      LANGFUSE_INIT_ORG_ID: ${LANGFUSE_INIT_ORG_ID:-}
      LANGFUSE_INIT_ORG_NAME: ${LANGFUSE_INIT_ORG_NAME:-}
      LANGFUSE_INIT_PROJECT_ID: ${LANGFUSE_INIT_PROJECT_ID:-}
      LANGFUSE_INIT_PROJECT_NAME: ${LANGFUSE_INIT_PROJECT_NAME:-}
      LANGFUSE_INIT_PROJECT_PUBLIC_KEY: ${LANGFUSE_INIT_PROJECT_PUBLIC_KEY:-}
      LANGFUSE_INIT_PROJECT_SECRET_KEY: ${LANGFUSE_INIT_PROJECT_SECRET_KEY:-}
      LANGFUSE_INIT_USER_EMAIL: ${LANGFUSE_INIT_USER_EMAIL:-}
      LANGFUSE_INIT_USER_NAME: ${LANGFUSE_INIT_USER_NAME:-}
      LANGFUSE_INIT_USER_PASSWORD: ${LANGFUSE_INIT_USER_PASSWORD:-}
  clickhouse:
    image: docker.io/clickhouse/clickhouse-server
    restart: always
    user: "101:101"
    environment:
      CLICKHOUSE_DB: default
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-clickhouse}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-clickhouse} # CHANGEME
    volumes:
      - langfuse_clickhouse_data:/var/lib/clickhouse
      - langfuse_clickhouse_logs:/var/log/clickhouse-server
    ports:
      - 127.0.0.1:8123:8123
      - 127.0.0.1:9000:9000
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 1s
  minio:
    image: docker.io/minio/minio
    restart: always
    entrypoint: sh
    # create the 'langfuse' bucket before starting the service
    command: -c 'mkdir -p /data/langfuse && minio server --address ":9000" --console-address ":9001" /data'
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minio}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-miniosecret} # CHANGEME
    ports:
      - 9990:9000
      - 127.0.0.1:9091:9001
    volumes:
      - langfuse_minio_data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 1s
      timeout: 5s
      retries: 5
      start_period: 1s
  redis:
    hostname: ${REDIS_HOST}
    image: docker.io/redis:${REDIS_VERSION}
    restart: always
    # CHANGEME: row below to secure redis password
    command: >
      --requirepass ${REDIS_AUTH}
    ports:
      - ${REDIS_PORT}:${REDIS_PORT}
    volumes:
      - redis:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 3s
      timeout: 10s
      retries: 10
  redis-insight:
    hostname: ${REDIS_INSIGHT_SERVICE_HOST}
    image: redis/redisinsight:${REDIS_INSIGHT_VERSION}
    ports:
      - ${REDIS_INSIGHT_SERVICE_PORT}:${REDIS_INSIGHT_INTERNAL_PORT}
    volumes:
      - redisinsight:/data
    environment:
      RI_APP_PORT: ${REDIS_INSIGHT_INTERNAL_PORT}
      RI_APP_HOST: ${REDIS_INSIGHT_LISTEN_ADDRESS}
      RI_REDIS_HOST: ${REDIS_HOST}
      RI_REDIS_PORT: ${REDIS_PORT}
      RI_REDIS_USERNAME: ${REDIS_USER}
      RI_REDIS_PASSWORD: ${REDIS_AUTH}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${REDIS_INSIGHT_INTERNAL_PORT}/api/health"]
      interval: 5s
      retries: 5
  n8n:
    hostname: ${N8N_SERVICE_HOST}
    image: docker.n8n.io/n8nio/n8n:${N8N_VERSION}
    profiles:
      - n8n
    deploy:
      resources:
        limits:
          cpus: ${N8N_CPUS}
          memory: ${N8N_MEM}
        reservations:
          memory: ${N8N_MEM}
    ports:
      - ${N8N_SERVICE_PORT}:${N8N_INTERNAL_PORT}
    environment:
      WEBHOOK_URL: http://${HOST_LOCAL_IP}:${N8N_SERVICE_PORT}
      #WEBHOOK_URL: ${N8N_INTERNET_URL}
      #N8N_HOST: ${N8N_INTERNAL_HOST}
      #N8N_PORT: ${N8N_INTERNAL_PORT}
      #N8N_LISTEN_ADDRESS: ${N8N_LISTEN_ADDRESS}
      N8N_EDITOR_BASE_URL: ${N8N_EDITOR_BASE_URL}
      #N8N_EDITOR_BASE_URL: ${N8N_INTERNET_URL}
      #N8N_PATH: ${N8N_PATH}
      #N8N_PUBLIC_API_ENDPOINT: ${N8N_PUBLIC_API_ENDPOINT}
      #N8N_ENDPOINT_WEBHOOK: ${N8N_ENDPOINT_WEBHOOK}
      #N8N_ENDPOINT_WEBHOOK_TEST: ${N8N_ENDPOINT_WEBHOOK_TEST}
      #N8N_ENDPOINT_WEBHOOK_WAIT: ${N8N_ENDPOINT_WEBHOOK_WAIT}
      N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS: "true"
      N8N_SECURE_COOKIE: "false"
      N8N_RUNNERS_ENABLED: "true"
    volumes:
      - n8n-data:/home/node/.n8n
  langflow:
    hostname: ${LANGFLOW_SERVICE_HOST}
    #image: langflowai/langflow:${LANGFLOW_VERSION}
    image: langflowai/langflow-nightly:${LANGFLOW_NIGHTLY_VERSION}
    deploy:
      resources:
        limits:
          cpus: ${LANGFLOW_CPUS}
          memory: ${LANGFLOW_MEM}
        reservations:
          memory: ${LANGFLOW_MEM}
    ports:
      - ${LANGFLOW_SERVICE_PORT}:${LANGFLOW_INTERNAL_PORT}
    environment:
      LANGFLOW_HOST: ${LANGFLOW_INTERNAL_HOST}
      LANGFLOW_PORT: ${LANGFLOW_INTERNAL_PORT}
      LANGFLOW_SUPERUSER: ${LANGFLOW_SUPERUSER}
      LANGFLOW_SUPERUSER_PASSWORD: ${LANGFLOW_SUPERUSER_PASSWORD}
      LANGFLOW_AUTO_LOGIN: "true"
      #LANGFLOW_CACHE_TYPE: redis
      LANGFLOW_CACHE_TYPE: memory
      LANGFLOW_REDIS_HOST: ${REDIS_HOST}
      LANGFLOW_REDIS_PORT: ${REDIS_PORT}
      LANGFLOW_REDIS_DB: 0
      LANGFLOW_REDIS_PASSWORD: ${REDIS_AUTH}
      LANGFLOW_REDIS_CACHE_EXPIRE: 3600
      LANGFLOW_COMPONENTS_PATH: /home
      LANGFLOW_WORKERS: 3
      LANGFLOW_DEACTIVATE_TRACING: "false"
      LANGFLOW_DATABASE_URL: postgresql://langflow-user:langflow-pass@${POSTGRESQL_AI_SERVICE_HOST}:${POSTGRESQL_AI_INTERNAL_PORT}/langflow
      LANGFLOW_CONFIG_DIR: /app/langflow
      LANGFLOW_CORS_ORIGINS: '*'
      LANGFLOW_LOG_LEVEL: "${LANGFLOW_LOG_LEVEL}"
      LANGFLOW_ENABLE_LOG_RETRIEVAL: "true"
      LANGFLOW_PRETTY_LOGS: "true"
      LANGFLOW_LOG_RETRIEVER_BUFFER_SIZE: 10000
      LANGFLOW_CREATE_STARTER_PROJECTS: "true"
      LANGFLOW_UPDATE_STARTER_PROJECTS: "true"
      LANGFLOW_LAZY_LOAD_COMPONENTS: "false"
      LANGFUSE_SECRET_KEY: "${LANGFUSE_SECRET_KEY}"
      LANGFUSE_PUBLIC_KEY: "${LANGFUSE_PUBLIC_KEY}"
      LANGFUSE_HOST: "http://${HOST_LOCAL_IP}:${LANGFUSE_WEB_INTERNAL_PORT}"
      LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT: "OLLAMA_URL"
      OLLAMA_URL: ${OLLAMA_SERVICE_HOST}:${OLLAMA_SERVICE_PORT}
    volumes:
      - ./langflow:/app/langflow:Z
      - ./langflow/flows:/home:Z
    depends_on:
      postgres-ai-server:
        condition: service_healthy
  docling-serve:
    hostname: ${DOCLING_SERVE_HOST}
    image: quay.io/docling-project/${DOCLING_SERVE_IMAGE}:${DOCLING_SERVE_VERSION}
    deploy:
      resources:
        limits:
          cpus: ${DOCLING_SERVE_CPUS}
          memory: ${DOCLING_SERVE_MEM}
        reservations:
          memory: ${DOCLING_SERVE_MEM}
    ports:
      - ${DOCLING_SERVE_SERVICE_PORT}:${DOCLING_SERVE_INTERNAL_PORT}
    environment:
      DOCLING_SERVE_ENABLE_UI: 1
  anythingllm:
    hostname: ${ANYTHINGLLM_SERVICE_HOST}
    image: mintplexlabs/anythingllm:${ANYTHINGLLM_VERSION}
    profiles:
      - anythingllm
    deploy:
      resources:
        limits:
          cpus: ${ANYTHINGLLM_CPUS}
          memory: ${ANYTHINGLLM_MEM}
        reservations:
          memory: ${ANYTHINGLLM_MEM}
    ports:
      - ${ANYTHINGLLM_SERVICE_PORT}:${ANYTHINGLLM_INTERNAL_PORT}
    volumes:
      - ./anythingllm/storage:/app/server/storage:Z
      - ./anythingllm/.env:/app/server/.env:Z
    environment:
      STORAGE_DIR: /app/server/storage
  localai:
    hostname: ${LOCALAI_SERVICE_HOST}
    image: localai/localai:${LOCALAI_VERSION}
    profiles:
      - localai
    deploy:
      resources:
        limits:
          cpus: ${LOCALAI_CPUS}
          memory: ${LOCALAI_MEM}
        reservations:
          memory: ${LOCALAI_MEM}
    ports:
      - ${LOCALAI_SERVICE_PORT}:${LOCALAI_INTERNAL_PORT}
    volumes:
      - ./localai/models:/models:Z
    environment:
      LOCALAI_EXTERNAL_BACKENDS: "llama-cpp"
      PRELOAD_MODELS: '[{"url": "","name": "gpt4all-j"}]'
      #MODELS: "huggingface://ibm-granite/granite-4.0-h-micro"
      #MODELS: "https://huggingface.co/ibm-granite/granite-4.0-h-micro"
      #MODELS: "qwen3-4b,llama-3.2-3b-instruct:q8_0,ibm-granite_granite-4.0-h-tiny,ibm-granite_granite-4.0-h-micro"
      #MODELS: "huggingface://Qwen/Qwen3-4B,huggingface://ibm-granite/granite-4.0-h-tiny,huggingface://ibm-granite/granite-4.0-h-micro"
#    entrypoint: ["/bin/bash", "-c", "\
#      local-ai models install qwen3-4b && \
#      local-ai models install llama-3.2-3b-instruct:q8_0 && \
#      local-ai models install ibm-granite_granite-4.0-h-tiny && \
#      local-ai models install ibm-granite_granite-4.0-h-micro && \
#      wait"]
  ollama:
    hostname: ${OLLAMA_SERVICE_HOST}
    image: ollama/ollama:${OLLAMA_VERSION}
    profiles:
      - ollama
    deploy:
      resources:
        limits:
          cpus: ${OLLAMA_CPUS}
          memory: ${OLLAMA_MEM}
        reservations:
          memory: ${OLLAMA_MEM}
    ports:
      - ${OLLAMA_SERVICE_PORT}:${OLLAMA_INTERNAL_PORT}
    volumes:
      - ./ollama:/root/.ollama:Z
    environment:
      OLLAMA_HOST: ${OLLAMA_INTERNAL_HOST}:${OLLAMA_INTERNAL_PORT}
      OLLAMA_ORIGINS: '*'
      OLLAMA_DEBUG: 2
    entrypoint: ["/bin/bash", "-c", "\
      ollama serve & \
      sleep 5 && \
      ollama pull ibm/granite4:micro-h && \
      ollama pull ibm/granite4:tiny-h && \
      ollama pull qwen3:4b && \
      wait"]
  mcp-context-forge:
    hostname: ${MCP_CONTEXT_FORGE_SERVICE_HOST}
    image: ghcr.io/ibm/mcp-context-forge:${MCP_CONTEXT_FORGE_VERSION}
    deploy:
      resources:
        limits:
          cpus: ${MCP_CONTEXT_FORGE_CPUS}
          memory: ${MCP_CONTEXT_FORGE_MEM}
        reservations:
          memory: ${MCP_CONTEXT_FORGE_MEM}
    ports:
      - ${MCP_CONTEXT_FORGE_SERVICE_PORT}:${MCP_CONTEXT_FORGE_INTERNAL_PORT}
    environment:
      MCPGATEWAY_UI_ENABLED: true
      MCPGATEWAY_ADMIN_API_ENABLED: true
      HOST: ${MCP_CONTEXT_FORGE_INTERNAL_HOST}
      PORT: ${MCP_CONTEXT_FORGE_INTERNAL_PORT}
      #APP_ROOT_PATH: ${MCP_CONTEXT_FORGE_ROOT_PATH}
      JWT_SECRET_KEY: my-test-key
      BASIC_AUTH_USER: admin
      BASIC_AUTH_PASSWORD: changeme
      AUTH_REQUIRED: true
      PLATFORM_ADMIN_EMAIL: admin@example.com
      PLATFORM_ADMIN_PASSWORD: changeme
      PLATFORM_ADMIN_FULL_NAME: "Platform Administrator"
      DATABASE_URL: postgresql://mcpgw-user:mcpgw-pass@${POSTGRESQL_AI_SERVICE_HOST}:${POSTGRESQL_AI_INTERNAL_PORT}/mcpgw
      SECURE_COOKIES: false
      PLUGINS_ENABLED: true
      # Enable internal observability
      OBSERVABILITY_ENABLED: true
      OBSERVABILITY_METRICS_ENABLED: true
      OBSERVABILITY_EVENTS_ENABLED: true
      # Automatically trace HTTP requests
      OBSERVABILITY_TRACE_HTTP_REQUESTS: true
      # Retention and limits
      OBSERVABILITY_TRACE_RETENTION_DAYS: 7
      OBSERVABILITY_MAX_TRACES: 100000
      # Trace sampling (1.0 = 100%, 0.1 = 10%)
      OBSERVABILITY_SAMPLE_RATE: 1.0
    depends_on:
      postgres-ai-server:
        condition: service_healthy
  bamoe-mcp-server:
    hostname: ${MCP_SERVER_SERVICE_HOST}
    image: quay.io/bamoe/mcp-server:${BAMOE_VERSION}
    deploy:
      resources:
        limits:
          cpus: ${MCP_SERVER_CPUS}
          memory: ${MCP_SERVER_MEM}
        reservations:
          memory: ${MCP_SERVER_MEM}
    ports:
      - ${MCP_SERVER_SERVICE_PORT}:${MCP_SERVER_INTERNAL_PORT}
    environment:
      MCP_SERVER_OPENAPI_URLS: "http://${BPMN_APP_SERVICE_HOST}:${BPMN_APP_INTERNAL_PORT}/q/openapi,http://${DMN_APP_SERVICE_HOST}:${DMN_APP_INTERNAL_PORT}/q/openapi"
      MCP_SERVER_PORT: ${MCP_SERVER_INTERNAL_PORT}
      QUARKUS_HTTP_HOST: ${MCP_SERVER_INTERNAL_HOST}
    depends_on:
      bamoe-bpmn-app:
        condition: service_healthy
      bamoe-dmn-app:
        condition: service_healthy
  bamoe-maven-repository:
    hostname: ${BAMOE_MAVEN_REPOSITORY_SERVICE_HOST}
    image: quay.io/bamoe/maven-repository:${BAMOE_VERSION}
    deploy:
      resources:
        limits:
          cpus: ${BAMOE_MAVEN_REPOSITORY_CPUS}
          memory: ${BAMOE_MAVEN_REPOSITORY_MEM}
        reservations:
          memory: ${BAMOE_MAVEN_REPOSITORY_MEM}
    profiles:
      - bamoe-maven-repository
    ports:
      - ${BAMOE_MAVEN_REPOSITORY_INTERNAL_PORT}:${BAMOE_MAVEN_REPOSITORY_SERVICE_PORT}
  bamoe-management-console:
    hostname: ${BAMOE_MANAGEMENT_CONSOLE_SERVICE_HOST}
    image: quay.io/bamoe/management-console:${BAMOE_VERSION}
    deploy:
      resources:
        limits:
          cpus: ${BAMOE_MANAGEMENT_CONSOLE_CPUS}
          memory: ${BAMOE_MANAGEMENT_CONSOLE_MEM}
        reservations:
          memory: ${BAMOE_MANAGEMENT_CONSOLE_MEM}
    ports:
      - ${BAMOE_MANAGEMENT_CONSOLE_SERVICE_PORT}:${BAMOE_MANAGEMENT_CONSOLE_INTERNAL_PORT}
    #depends_on:
    #  keycloak:
    #    condition: service_healthy
    environment:
      #RUNTIME_TOOLS_MANAGEMENT_CONSOLE_OIDC_CLIENT_CLIENT_ID: kie-management-console
      RUNTIME_TOOLS_MANAGEMENT_CONSOLE_BASE_PATH: bamoe-process-management-console
      RUNTIME_TOOLS_MANAGEMENT_CONSOLE_USE_APACHE_HTTPD_BASE_PATH_ALIAS: "true"
      #RUNTIME_TOOLS_MANAGEMENT_CONSOLE_MANAGED_BUSINESS_SERVICES: '[{"name":"Kogito Secure App", "businessServiceUrl":"http://localhost:8081", "clientId":"kie-management-console"},{"name":"Kogito Unsecure App", "businessServiceUrl":"http://localhost:8082"}]'
  postgres-server:
    hostname: ${POSTGRESQL_SERVICE_HOST}
    image: 'postgres:${POSTGRESQL_VERSION}'
    deploy:
      resources:
        limits:
          cpus: ${POSTGRESQL_CPUS}
          memory: ${POSTGRESQL_MEM}
        reservations:
          memory: ${POSTGRESQL_MEM}
    ports:
      - ${POSTGRESQL_SERVICE_PORT}:${POSTGRESQL_INTERNAL_PORT}
    volumes:
      - './sql/postgres-server:/docker-entrypoint-initdb.d:Z'
      - './sql/postgres-server/postgresql.conf:/etc/postgresql.conf'
      - postgres-data:/var/lib/postgresql/data
    command:
      postgres -c config_file=/etc/postgresql.conf
    healthcheck:
      test:
        - CMD
        - pg_isready
        - '-q'
        - '-d'
        - kogito
        - '-U'
        - kogito-user
      timeout: 45s
      interval: 10s
      retries: 50
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
  postgres-ai-server:
    hostname: ${POSTGRESQL_AI_SERVICE_HOST}
    build:
      context: pgvector
      dockerfile: Dockerfile
    deploy:
      resources:
        limits:
          cpus: ${POSTGRESQL_AI_CPUS}
          memory: ${POSTGRESQL_AI_MEM}
        reservations:
          memory: ${POSTGRESQL_AI_MEM}
    ports:
      - ${POSTGRESQL_AI_SERVICE_PORT}:${POSTGRESQL_AI_INTERNAL_PORT}
    volumes:
      - './sql/postgres-ai-server:/docker-entrypoint-initdb.d:Z'
      - './sql/postgres-ai-server/postgresql.conf:/etc/postgresql.conf'
      - postgres-ai-data:/var/lib/postgresql/data
    command:
      postgres -c config_file=/etc/postgresql.conf
    healthcheck:
      test:
        - CMD
        - pg_isready
        - '-q'
        - '-d'
        - postgres
        - '-U'
        - postgres
      timeout: 45s
      interval: 10s
      retries: 50
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
  pgadmin:
    image: dpage/pgadmin4:${PGADMIN_VERSION}
    hostname: ${PGADMIN_SERVICE_HOST}
    deploy:
      resources:
        limits:
          cpus: ${PGADMIN_CPUS}
          memory: ${PGADMIN_MEM}
        reservations:
          memory: ${PGADMIN_MEM}
    ports:
      - ${PGADMIN_SERVICE_PORT}:${PGADMIN_INTERNAL_PORT}
    volumes:
      - ./pgadmin/servers.json:/pgadmin4/servers.json:Z
      - ./pgadmin/pgpass:/pgadmin4/pgpass:Z
    entrypoint: >
      /bin/sh -c "
      cp -f /pgadmin4/pgpass /var/lib/pgadmin/;
      chmod 600 /var/lib/pgadmin/pgpass;
      /entrypoint.sh
      "
    environment:
      PGADMIN_DEFAULT_EMAIL: user@kogito.org
      PGADMIN_DEFAULT_PASSWORD: pass
      PGADMIN_CONFIG_SERVER_MODE: "False"
      PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED: "False"
      GUNICORN_ACCESS_LOGFILE: "/dev/null"
  keycloak:
    hostname: ${KEYCLOAK_SERVICE_HOST}
    image: quay.io/keycloak/keycloak:${KEYCLOAK_VERSION}
    deploy:
      resources:
        limits:
          cpus: ${KEYCLOAK_CPUS}
          memory: ${KEYCLOAK_MEM}
        reservations:
          memory: ${KEYCLOAK_MEM}
    profiles:
      - keycloak
    ports:
      - ${KEYCLOAK_SERVICE_PORT}:${KEYCLOAK_INTERNAL_PORT}
    depends_on:
      postgres-supporting-services:
        condition: service_healthy
    volumes:
      - ./keycloak:/opt/keycloak/data/import:z
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "exec 3<>/dev/tcp/127.0.0.1/9000; echo -e 'GET /health/ready HTTP/1.1\r\nHost: localhost:9000\r\nConnection: close\r\n\r\n' >&3;cat <&3 | grep -q '\"status\": \"UP\"' && exit 0 || exit 1",
        ]
      interval: 2s
      timeout: 1s
      retries: 50
    environment:
      KC_HEALTH_ENABLED: "true"
      DB_VENDOR: POSTGRES
      DB_ADDR: ${POSTGRESQL_SERVICE_HOST}
      DB_DATABASE: keycloak
      DB_USER: kogito-user
      DB_SCHEMA: public
      DB_PASSWORD: kogito-pass
      KEYCLOAK_USER: admin
      KEYCLOAK_PASSWORD: admin
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
    command:
      start-dev --import-realm --hostname-port=${KEYCLOAK_SERVICE_PORT} --health-enabled=true
  redpanda:
    hostname: ${REDPANDA_SERVICE_HOST}
    image: redpandadata/redpanda:${REDPANDA_VERSION}
    deploy:
      resources:
        limits:
          cpus: ${REDPANDA_CPUS}
          memory: ${REDPANDA_MEM}
        reservations:
          memory: ${REDPANDA_MEM}
    ports:
      - 18081:18081
      - 18082:18082
      - 14001:8081
      - 14002:8082
      - 9092:9092
      - 28082:28082
      - 29092:2909
      - 9093:9093
      - 9644:9644
    volumes:
      - redpanda_data:/var/lib/redpanda/data
      #- ./redpanda/bootstrap.yml:/etc/redpanda/.bootstrap.yaml
    #environment:
      #DATA_TRANSFORMS_ENABLED: "true"
    command:
    - redpanda
    - start
    - --smp
    - '6'
    - --reserve-memory
    - 0M
    - --overprovisioned
    - --node-id
    - '0'
    - --kafka-addr
    - PLAINTEXT://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
    - --advertise-kafka-addr
    - PLAINTEXT://redpanda:9093,OUTSIDE://localhost:9092
    - --pandaproxy-addr
    - PLAINTEXT://0.0.0.0:28082,OUTSIDE://0.0.0.0:8082
    - --advertise-pandaproxy-addr
    - PLAINTEXT://redpanda:28082,OUTSIDE://localhost:8082
    - --schema-registry-addr internal://0.0.0.0:8081,external://0.0.0.0:18081
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8082/brokers" ]
      interval: 1s
      timeout: 1s
      retries: 50
  redpanda-console:
    hostname: ${REDPANDA_CONSOLE_SERVICE_HOST}
    image: redpandadata/console:${REDPANDA_CONSOLE_VERSION}
    deploy:
      resources:
        limits:
          cpus: ${REDPANDA_CONSOLE_CPUS}
          memory: ${REDPANDA_CONSOLE_MEM}
        reservations:
          memory: ${REDPANDA_CONSOLE_MEM}   
    profiles:
      - full
      - messaging-redpanda
    # mount the local directory that contains your license key to the container.
    # give Redpanda Console read access to the license.
    volumes:
    # Remove ro?
      - ./license:/etc/redpanda:ro
    entrypoint: /bin/sh
    command: -c 'echo "$$CONSOLE_CONFIG_FILE" > /tmp/config.yml && echo "$$CONSOLE_ROLEBINDINGS_CONFIG_FILE" > /tmp/role-bindings.yml && /app/console'
    environment:
      #REDPANDA_LICENSE_FILEPATH: /etc/redpanda/redpanda.license
      CONFIG_FILEPATH: /tmp/config.yml
      CONSOLE_CONFIG_FILE: |
        kafka:
          brokers: ["redpanda:9093"]
        schemaRegistry:
          enabled: true
          urls: ["http://redpanda:8081"]
        redpanda:
          adminApi:
            enabled: true
            urls: ["http://redpanda:9644"]
        kafkaConnect:
          enabled: false
          clusters:
            - name: datagen
              url: http://redpanda-connect:8083
        authorization:
          roleBindings:
            - roleName: admin
              users:
                - loginType: basic
                  name: "admin"
    ports:
      - ${REDPANDA_CONSOLE_SERVICE_PORT}:${REDPANDA_CONSOLE_INTERNAL_PORT}
    depends_on:
      - redpanda
  wiremock-studio:
    hostname: ${WIREMOCK_SERVICE_HOST}
    image: jizuzquiza/wiremock-studio:${WIREMOCK_VERSION}
    profiles:
      - wiremock
    deploy:
      resources:
        limits:
          cpus: ${WIREMOCK_CPUS}
          memory: ${WIREMOCK_MEM}
        reservations:
          memory: ${WIREMOCK_MEM}
    volumes:
      - ./wiremock-data-storage:/home/wiremock:z
    ports:
    - ${WIREMOCK_SERVICE_PORT}:${WIREMOCK_INTERNAL_PORT}
  bamoe-bpmn-app:
    hostname: ${BPMN_APP_SERVICE_HOST}
    image: ${BPMN_APP_DOCKER_IMAGE_REGISTRY}/${BPMN_APP_DOCKER_IMAGE_GROUP}/bamoe-bpmn-app:${BPMN_APP_VERSION}
    deploy:
      resources:
        limits:
          cpus: ${BPMN_APP_CPUS}
          memory: ${BPMN_APP_MEM}
        reservations:
          memory: ${BPMN_APP_MEM}
    ports:
    - ${BPMN_APP_SERVICE_PORT}:${BPMN_APP_INTERNAL_PORT}
    environment:
      JAVA_OPTS: >
        -XX:ActiveProcessorCount=2
        -XX:MaxRAMPercentage=80.0
        -XX:+UseG1GC
        -XX:MinHeapFreeRatio=10
        -XX:MaxHeapFreeRatio=20
        -XX:GCTimeRatio=4
        -XX:AdaptiveSizePolicyWeight=90
        -XX:+ExitOnOutOfMemoryError
        -XX:+HeapDumpOnOutOfMemoryError
        -XX:HeapDumpPath=/home/jboss
        -Djava.util.logging.manager=org.jboss.logmanager.LogManager
        -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:8558
        -Dcom.sun.management.jmxremote
        -Dcom.sun.management.jmxremote.port=12346
        -Dcom.sun.management.jmxremote.rmi.port=12346
        -Dcom.sun.management.jmxremote.authenticate=false
        -Dcom.sun.management.jmxremote.ssl=false
        -Dcom.sun.management.jmxremote.host=0.0.0.0
        -Djava.rmi.server.hostname=localhost
      KOGITO_PERSISTENCE_PROTO_MARSHALLER: "false"
      QUARKUS_HTTP_CORS_ORIGINS: '*'
      QUARKUS_HTTP_HOST: ${BPMN_APP_INTERNAL_HOST}
      QUARKUS_HTTP_PORT: ${BPMN_APP_INTERNAL_PORT}
      GC_CONTAINER_OPTIONS: "-XX:+UseG1GC"
      # The number if IO threads used to perform IO. This will be automatically set to a reasonable value based on the number of CPU cores if it is not provided.
      # If this is set to a higher value than the number of Vert.x event loops then it will be capped at the number of event loops.
      # In general this should be controlled by setting quarkus.vertx.event-loops-pool-size, this setting should only be used if you want to limit the number
      # of HTTP io threads to a smaller number than the total number of IO threads.
      # QUARKUS_HTTP_IO_THREADS: 5
      # The number of event loops. By default, it matches the number of CPUs detected on the system.
      # QUARKUS_VERTX_EVENT_LOOPS_POOL_SIZE: 5
      # The maximum number of threads. If this is not specified then it will be automatically sized to the greatest of 8 * the number of available processors and 200.
      # For example if there are 4 processors the max threads will be 200. If there are 48 processors it will be 384.
      QUARKUS_THREAD_POOL_MAX_THREADS: 300
      # The core thread pool size. This number of threads will always be kept alive. Defaults to 1
      QUARKUS_THREAD_POOL_CORE_THREADS: 1
      # The amount of time a thread will stay alive with no work. Defaults to 30S
      QUARKUS_THREAD_POOL_KEEP_ALIVE_TIME: 30S
      # See https://quarkus.io/guides/datasource#jdbc-configuration
      QUARKUS_DATASOURCE_JDBC_URL: 'jdbc:postgresql://${POSTGRESQL_SERVICE_HOST}:${POSTGRESQL_SERVICE_PORT}/kogito'
      QUARKUS_DATASOURCE_USERNAME: kogito-user
      QUARKUS_DATASOURCE_PASSWORD: kogito-pass
      QUARKUS_DATASOURCE_DB_KIND: postgresql
      QUARKUS_DATASOURCE_JDBC_ENABLE_METRICS: "true"
      QUARKUS_DATASOURCE_JDBC_METRICS_ENABLED: "true"
      # XA transaction objects datasource
      QUARKUS_DATASOURCE__XOS__JDBC_URL: 'jdbc:postgresql://${POSTGRESQL_SERVICE_HOST}:${POSTGRESQL_SERVICE_PORT}/xa-object-store'
      QUARKUS_DATASOURCE__XOS__USERNAME: kogito-user
      QUARKUS_DATASOURCE__XOS__PASSWORD: kogito-pass
      QUARKUS_DATASOURCE__XOS__DB_KIND: postgresql
      QUARKUS.DATASOURCE__XOS__JDBC_TRANSACTIONS: disabled
      # The initial size of the pool. Usually you will want to set the initial size to match at least the minimal size, 
      # but this is not enforced so to allow for architectures which prefer a lazy initialization of the connections on boot, 
      # while being able to sustain a minimal pool size after boot. DEFAULT: null
      QUARKUS_DATASOURCE_JDBC_INITIAL_SIZE: 20
      # The datasource pool minimum size. DEFAULT: 0
      QUARKUS_DATASOURCE_JDBC_MIN_SIZE: 20
      # The datasource pool maximum size. DEFAULT: 20
      QUARKUS_DATASOURCE_JDBC_MAX_SIZE: 80
      # The interval at which we validate idle connections in the background. Set to 0 to disable background validation. DEFAULT: 2M
      # QUARKUS_DATASOURCE_JDBC_BACKGROUND_VALIDATION_INTERVAL: 2M
      # Perform foreground validation on connections that have been idle for longer than the specified interval. DEFAULT: null
      # QUARKUS_DATASOURCE_JDBC_FOREGROUND_VALIDATION_INTERVAL: 15S
      # The timeout before cancelling the acquisition of a new connection. DEFAULT: 5S
      # QUARKUS_DATASOURCE_JDBC_ACQUISITION_TIMEOUT: 5S
      # The interval at which we check for connection leaks. DEFAULT: disabled
      # QUARKUS_DATASOURCE_JDBC_LEAK_DETECTION_INTERVAL: 10M
      # The interval at which we try to remove idle connections. DEFAULT: 5M
      # QUARKUS_DATASOURCE_JDBC_IDLE_REMOVAL_INTERVAL: 5M
      # The max lifetime of a connection. DEFAULT: disabled
      # QUARKUS_DATASOURCE_JDBC_MAX_LIFETIME: 1M
      # The transaction isolation level. DEFAULT: null
      # QUARKUS_DATASOURCE_JDBC_TRANSACTION_ISOLATION_LEVEL: undefined, none, read-uncommitted, read-committed, repeatable-read, serializable
      QUARKUS_TRANSACTION-MANAGER_ENABLE-RECOVERY: "true"
      QUARKUS_TRANSACTION-MANAGER_OBJECT-STORE_DATASOURCE: "xos"
      QUARKUS_TRANSACTION-MANAGER_OBJECT-STORE_TYPE: "jdbc"
      QUARKUS_TRANSACTION-MANAGER_OBJECT-STORE_CREATE-TABLE: "true"
      KOGITO_SERVICE_URL: 'http://${BPMN_APP_SERVICE_HOST}:${BPMN_APP_SERVICE_PORT}'
      KOGITO_DATAINDEX_HTTP_URL: 'http://${BPMN_APP_SERVICE_HOST}:${BPMN_APP_SERVICE_PORT}'
      KOGITO_JOBS_SERVICE_URL: 'http://${BPMN_APP_SERVICE_HOST}:${BPMN_APP_SERVICE_PORT}'
      # The current chunk size in minutes the scheduler handles, it is used to keep a limit number of jobs scheduled
      # in the in-memory scheduler.
      #KOGITO_JOBS-SERVICE_SCHEDULERCHUNKINMINUTES: 10
      # The interval the job loading method runs to fetch the persisted jobs from the repository.
      #KOGITO_JOBS-SERVICE_LOADJOBINTERVALINMINUTES: 10
      # The interval based on the current time the job loading method uses to fetch jobs "FROM (now -
      # {@link #loadJobFromCurrentTimeIntervalInMinutes}) TO {@link #schedulerChunkInMinutes}"
      #KOGITO_JOBS-SERVICE_LOADJOBFROMCURRENTTIMEINTERVALINMINUTES: 0
      # Number of retries configured for the periodic jobs loading procedure. Every time the procedure is started this
      # value is considered.
      KOGITO_JOBS-SERVICE_LOADJOBRETRIES: 3
      # Error strategy to apply when the periodic jobs loading procedure has exceeded the jobLoadReties. NONE, FAIL_SERVICE
      #KOGITO_JOBS-SERVICE_LOADJOBERRORSTRATEGY: FAIL_SERVICE
      KOGITO_JOBS-SERVICE_FORCEEXECUTEEXPIREDJOBS: "true"
      KOGITO_JOBS-SERVICE_FORCEEXECUTEEXPIREDJOBSONSERVICESTART: "true"
      KOGITO_JOBS-SERVICE_NUMBEROFWORKERTHREADS: 10 
      KOGITO_JOBS-SERVICE_MAXNUMBEROFRETRIES: 12
      KOGITO_JOBS-SERVICE_RETRYMILLS: 3000
      KOGITO_JOBS-SERVICE_SCHEDULERCHUNKINMINUTES: 5
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSINSTANCES-EVENTS_CONNECTOR: smallrye-kafka
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSINSTANCES-EVENTS_TOPIC: kogito-processinstances-events
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSINSTANCES-EVENTS_VALUE_SERIALIZER: org.apache.kafka.common.serialization.StringSerializer
      MP_MESSAGING_OUTGOING_KOGITO-USERTASKINSTANCES-EVENTS_CONNECTOR: smallrye-kafka
      MP_MESSAGING_OUTGOING_KOGITO-USERTASKINSTANCES-EVENTS_TOPIC: kogito-usertaskinstances-events
      MP_MESSAGING_OUTGOING_KOGITO-USERTASKINSTANCES-EVENTS_VALUE_SERIALIZER: org.apache.kafka.common.serialization.StringSerializer
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSDEFINITIONS-EVENTS_CONNECTOR: smallrye-kafka
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSDEFINITIONS-EVENTS_TOPIC: kogito-processdefinitions-events
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSDEFINITIONS-EVENTS_VALUE_SERIALIZER: org.apache.kafka.common.serialization.StringSerializer
      KOGITO_EVENTS_GROUPING: "false"
      KIE_FLYWAY_ENABLED: "true"
      QUARKUS_REST_CLIENT__DUMMY_REST_SERVICE__URL: http://localhost:19002
    depends_on:
      postgres-server:
        condition: service_healthy
      redpanda:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://${BPMN_APP_INTERNAL_HOST}:${BPMN_APP_INTERNAL_PORT}/q/health/ready"]
      interval: 5s
      retries: 5
  bamoe-dmn-app:
    hostname: ${DMN_APP_SERVICE_HOST}
    image: ${DMN_APP_DOCKER_IMAGE_REGISTRY}/${DMN_APP_DOCKER_IMAGE_GROUP}/bamoe-dmn-app:${DMN_APP_VERSION}
    deploy:
      resources:
        limits:
          cpus: ${DMN_APP_CPUS}
          memory: ${DMN_APP_MEM}
        reservations:
          memory: ${DMN_APP_MEM}
    ports:
      - ${DMN_APP_SERVICE_PORT}:${DMN_APP_INTERNAL_PORT}
    environment:
      QUARKUS_HTTP_CORS_ORIGINS: /.*/
      GC_CONTAINER_OPTIONS: "-XX:+UseG1GC"
      QUARKUS_HTTP_HOST: ${DMN_APP_INTERNAL_HOST}
      QUARKUS_HTTP_PORT: ${DMN_APP_INTERNAL_PORT}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://${DMN_APP_INTERNAL_HOST}:${DMN_APP_INTERNAL_PORT}/q/health/ready"]
      interval: 5s
      retries: 5
volumes:
    redpanda_data: {}
    n8n-data: {}
    postgres-data: {}
    postgres-ai-data: {}
    redis: {}
    redisinsight: {}
    langfuse_clickhouse_data: {}
    langfuse_clickhouse_logs: {}
    langfuse_minio_data: {}
