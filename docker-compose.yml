services:
  nginx:
    hostname: ${NGINX_SERVICE_HOST}
    image: nginx:${NGINX_VERSION}
    deploy:
      resources:
        limits:
          cpus: ${NGINX_CPUS}
          memory: ${NGINX_MEM}
        reservations:
          memory: ${NGINX_MEM}
    profiles:
      - nginx
    volumes:
    - ./nginx/templates:/etc/nginx/templates:Z
    ports:
      - ${NGINX_SERVICE_PORT}:${NGINX_INTERNAL_PORT}
    environment:
    - NGINX_HOST=${NGINX_INTERNAL_HOST}
    - NGINX_PORT=${NGINX_INTERNAL_PORT}
    depends_on:
      langflow:
        condition: service_started
  langfuse-worker:
    hostname: ${LANGFUSE_WORKER_SERVICE_HOST}
    image: docker.io/langfuse/langfuse-worker:${LANGFUSE_VERSION}
    profiles:
      - core
    depends_on: &langfuse-depends-on
      postgres-ai-server:
        condition: service_healthy
      minio:
        condition: service_healthy
      redis:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    ports:
      - 127.0.0.1:${LANGFUSE_WORKER_SERVICE_PORT}:${LANGFUSE_WORKER_INTERNAL_PORT}
    environment: &langfuse-worker-env
      HOSTNAME: ${LANGFUSE_WORKER_INTERNAL_HOST}
      NEXTAUTH_URL: ${NEXTAUTH_URL:-http://${HOST_LOCAL_IP}:${LANGFUSE_WEB_SERVICE_PORT}}
      DATABASE_URL: ${DATABASE_URL:-postgresql://langfuse-user:langfuse-pass@${POSTGRESQL_AI_SERVICE_HOST}:${POSTGRESQL_AI_INTERNAL_PORT}/langfuse} # CHANGEME
      SALT: ${SALT:-mysalt} # CHANGEME
      ENCRYPTION_KEY: ${ENCRYPTION_KEY:-0000000000000000000000000000000000000000000000000000000000000000} # CHANGEME: generate via `openssl rand -hex 32`
      TELEMETRY_ENABLED: ${TELEMETRY_ENABLED:-true}
      LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES: ${LANGFUSE_ENABLE_EXPERIMENTAL_FEATURES:-true}
      CLICKHOUSE_MIGRATION_URL: ${CLICKHOUSE_MIGRATION_URL:-clickhouse://clickhouse:9000}
      CLICKHOUSE_URL: ${CLICKHOUSE_URL:-http://clickhouse:8123}
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-clickhouse}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-clickhouse} # CHANGEME
      CLICKHOUSE_CLUSTER_ENABLED: ${CLICKHOUSE_CLUSTER_ENABLED:-false}
      LANGFUSE_USE_AZURE_BLOB: ${LANGFUSE_USE_AZURE_BLOB:-false}
      LANGFUSE_S3_EVENT_UPLOAD_BUCKET: ${LANGFUSE_S3_EVENT_UPLOAD_BUCKET:-langfuse}
      LANGFUSE_S3_EVENT_UPLOAD_REGION: ${LANGFUSE_S3_EVENT_UPLOAD_REGION:-auto}
      LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID: ${LANGFUSE_S3_EVENT_UPLOAD_ACCESS_KEY_ID:-minio}
      LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY: ${LANGFUSE_S3_EVENT_UPLOAD_SECRET_ACCESS_KEY:-miniosecret} # CHANGEME
      LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT: ${LANGFUSE_S3_EVENT_UPLOAD_ENDPOINT:-http://minio:9000}
      LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE: ${LANGFUSE_S3_EVENT_UPLOAD_FORCE_PATH_STYLE:-true}
      LANGFUSE_S3_EVENT_UPLOAD_PREFIX: ${LANGFUSE_S3_EVENT_UPLOAD_PREFIX:-events/}
      LANGFUSE_S3_MEDIA_UPLOAD_BUCKET: ${LANGFUSE_S3_MEDIA_UPLOAD_BUCKET:-langfuse}
      LANGFUSE_S3_MEDIA_UPLOAD_REGION: ${LANGFUSE_S3_MEDIA_UPLOAD_REGION:-auto}
      LANGFUSE_S3_MEDIA_UPLOAD_ACCESS_KEY_ID: ${LANGFUSE_S3_MEDIA_UPLOAD_ACCESS_KEY_ID:-minio}
      LANGFUSE_S3_MEDIA_UPLOAD_SECRET_ACCESS_KEY: ${LANGFUSE_S3_MEDIA_UPLOAD_SECRET_ACCESS_KEY:-miniosecret} # CHANGEME
      LANGFUSE_S3_MEDIA_UPLOAD_ENDPOINT: ${LANGFUSE_S3_MEDIA_UPLOAD_ENDPOINT:-http://localhost:9090}
      LANGFUSE_S3_MEDIA_UPLOAD_FORCE_PATH_STYLE: ${LANGFUSE_S3_MEDIA_UPLOAD_FORCE_PATH_STYLE:-true}
      LANGFUSE_S3_MEDIA_UPLOAD_PREFIX: ${LANGFUSE_S3_MEDIA_UPLOAD_PREFIX:-media/}
      LANGFUSE_S3_BATCH_EXPORT_ENABLED: ${LANGFUSE_S3_BATCH_EXPORT_ENABLED:-false}
      LANGFUSE_S3_BATCH_EXPORT_BUCKET: ${LANGFUSE_S3_BATCH_EXPORT_BUCKET:-langfuse}
      LANGFUSE_S3_BATCH_EXPORT_PREFIX: ${LANGFUSE_S3_BATCH_EXPORT_PREFIX:-exports/}
      LANGFUSE_S3_BATCH_EXPORT_REGION: ${LANGFUSE_S3_BATCH_EXPORT_REGION:-auto}
      LANGFUSE_S3_BATCH_EXPORT_ENDPOINT: ${LANGFUSE_S3_BATCH_EXPORT_ENDPOINT:-http://minio:9000}
      LANGFUSE_S3_BATCH_EXPORT_EXTERNAL_ENDPOINT: ${LANGFUSE_S3_BATCH_EXPORT_EXTERNAL_ENDPOINT:-http://localhost:9090}
      LANGFUSE_S3_BATCH_EXPORT_ACCESS_KEY_ID: ${LANGFUSE_S3_BATCH_EXPORT_ACCESS_KEY_ID:-minio}
      LANGFUSE_S3_BATCH_EXPORT_SECRET_ACCESS_KEY: ${LANGFUSE_S3_BATCH_EXPORT_SECRET_ACCESS_KEY:-miniosecret} # CHANGEME
      LANGFUSE_S3_BATCH_EXPORT_FORCE_PATH_STYLE: ${LANGFUSE_S3_BATCH_EXPORT_FORCE_PATH_STYLE:-true}
      LANGFUSE_INGESTION_QUEUE_DELAY_MS: ${LANGFUSE_INGESTION_QUEUE_DELAY_MS:-}
      LANGFUSE_INGESTION_CLICKHOUSE_WRITE_INTERVAL_MS: ${LANGFUSE_INGESTION_CLICKHOUSE_WRITE_INTERVAL_MS:-}
      REDIS_HOST: ${REDIS_HOST}
      REDIS_PORT: ${REDIS_PORT}
      REDIS_AUTH: ${REDIS_AUTH}
      REDIS_TLS_ENABLED: ${REDIS_TLS_ENABLED:-false}
      REDIS_TLS_CA: ${REDIS_TLS_CA:-/certs/ca.crt}
      REDIS_TLS_CERT: ${REDIS_TLS_CERT:-/certs/redis.crt}
      REDIS_TLS_KEY: ${REDIS_TLS_KEY:-/certs/redis.key}
      EMAIL_FROM_ADDRESS: ${EMAIL_FROM_ADDRESS:-}
      SMTP_CONNECTION_URL: ${SMTP_CONNECTION_URL:-}
  langfuse-web:
    hostname: ${LANGFUSE_WEB_SERVICE_HOST}
    image: docker.io/langfuse/langfuse:${LANGFUSE_VERSION}
    profiles:
      - core
    depends_on: *langfuse-depends-on
    ports:
      - ${LANGFUSE_WEB_SERVICE_PORT}:${LANGFUSE_WEB_INTERNAL_PORT}
    environment:
      <<: *langfuse-worker-env
      NEXTAUTH_SECRET: ${NEXTAUTH_SECRET:-mysecret} # CHANGEME
      HOSTNAME: ${LANGFUSE_WEB_INTERNAL_HOST}
      LANGFUSE_INIT_ORG_ID: ${LANGFUSE_INIT_ORG_ID:-}
      LANGFUSE_INIT_ORG_NAME: ${LANGFUSE_INIT_ORG_NAME:-}
      LANGFUSE_INIT_PROJECT_ID: ${LANGFUSE_INIT_PROJECT_ID:-}
      LANGFUSE_INIT_PROJECT_NAME: ${LANGFUSE_INIT_PROJECT_NAME:-}
      LANGFUSE_INIT_PROJECT_PUBLIC_KEY: ${LANGFUSE_INIT_PROJECT_PUBLIC_KEY:-}
      LANGFUSE_INIT_PROJECT_SECRET_KEY: ${LANGFUSE_INIT_PROJECT_SECRET_KEY:-}
      LANGFUSE_INIT_USER_EMAIL: ${LANGFUSE_INIT_USER_EMAIL:-}
      LANGFUSE_INIT_USER_NAME: ${LANGFUSE_INIT_USER_NAME:-}
      LANGFUSE_INIT_USER_PASSWORD: ${LANGFUSE_INIT_USER_PASSWORD:-}
  clickhouse:
    image: docker.io/clickhouse/clickhouse-server
    profiles:
      - core
    user: "101:101"
    environment:
      CLICKHOUSE_DB: default
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-clickhouse}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-clickhouse} # CHANGEME
    volumes:
      - langfuse_clickhouse_data:/var/lib/clickhouse
      - langfuse_clickhouse_logs:/var/log/clickhouse-server
    ports:
      - 8123:8123
      - 9000:9000
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 1s
  minio:
    image: cgr.dev/chainguard/minio:${MINIO_VERSION}
    profiles:
      - core
    entrypoint: sh
    # create the 'langfuse' bucket before starting the service
    command: -c 'mkdir -p /data/langfuse && minio server --address ":9000" --console-address ":9001" /data'
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minio}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-miniosecret} # CHANGEME
    ports:
      - 9990:9000
      - 9091:9001
    volumes:
      - langfuse_minio_data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 1s
      timeout: 5s
      retries: 5
      start_period: 1s
  redis:
    hostname: ${REDIS_HOST}
    image: docker.io/redis:${REDIS_VERSION}
    profiles:
      - core
    # CHANGEME: row below to secure redis password
    command: >
      --requirepass ${REDIS_AUTH}
    ports:
      - ${REDIS_PORT}:${REDIS_PORT}
    volumes:
      - redis:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 3s
      timeout: 10s
      retries: 10
  redis-insight:
    hostname: ${REDIS_INSIGHT_SERVICE_HOST}
    image: redis/redisinsight:${REDIS_INSIGHT_VERSION}
    profiles:
      - core
    ports:
      - ${REDIS_INSIGHT_SERVICE_PORT}:${REDIS_INSIGHT_INTERNAL_PORT}
    volumes:
      - redisinsight:/data
    environment:
      RI_APP_PORT: ${REDIS_INSIGHT_INTERNAL_PORT}
      RI_APP_HOST: ${REDIS_INSIGHT_LISTEN_ADDRESS}
      RI_REDIS_HOST: ${REDIS_HOST}
      RI_REDIS_PORT: ${REDIS_PORT}
      RI_REDIS_USERNAME: ${REDIS_USER}
      RI_REDIS_PASSWORD: ${REDIS_AUTH}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${REDIS_INSIGHT_INTERNAL_PORT}/api/health"]
      interval: 5s
      retries: 5
  n8n:
    hostname: ${N8N_SERVICE_HOST}
    image: docker.n8n.io/n8nio/n8n:${N8N_VERSION}
    profiles:
      - n8n
    deploy:
      resources:
        limits:
          cpus: ${N8N_CPUS}
          memory: ${N8N_MEM}
        reservations:
          memory: ${N8N_MEM}
    ports:
      - ${N8N_SERVICE_PORT}:${N8N_INTERNAL_PORT}
    environment:
      WEBHOOK_URL: http://${HOST_LOCAL_IP}:${N8N_SERVICE_PORT}
      #WEBHOOK_URL: ${N8N_INTERNET_URL}
      #N8N_HOST: ${N8N_INTERNAL_HOST}
      #N8N_PORT: ${N8N_INTERNAL_PORT}
      #N8N_LISTEN_ADDRESS: ${N8N_LISTEN_ADDRESS}
      N8N_EDITOR_BASE_URL: ${N8N_EDITOR_BASE_URL}
      #N8N_EDITOR_BASE_URL: ${N8N_INTERNET_URL}
      #N8N_PATH: ${N8N_PATH}
      #N8N_PUBLIC_API_ENDPOINT: ${N8N_PUBLIC_API_ENDPOINT}
      #N8N_ENDPOINT_WEBHOOK: ${N8N_ENDPOINT_WEBHOOK}
      #N8N_ENDPOINT_WEBHOOK_TEST: ${N8N_ENDPOINT_WEBHOOK_TEST}
      #N8N_ENDPOINT_WEBHOOK_WAIT: ${N8N_ENDPOINT_WEBHOOK_WAIT}
      N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS: "true"
      N8N_SECURE_COOKIE: "false"
      N8N_RUNNERS_ENABLED: "true"
    volumes:
      - n8n-data:/home/node/.n8n
  langflow:
    hostname: ${LANGFLOW_SERVICE_HOST}
    image: langflowai/langflow:${LANGFLOW_VERSION}
    #image: langflowai/langflow-nightly:${LANGFLOW_NIGHTLY_VERSION}
    profiles:
      - core
    #user: root
    deploy:
      resources:
        limits:
          cpus: ${LANGFLOW_CPUS}
          memory: ${LANGFLOW_MEM}
        reservations:
          memory: ${LANGFLOW_MEM}
    ports:
      - ${LANGFLOW_SERVICE_PORT}:${LANGFLOW_INTERNAL_PORT}
    environment:
      LANGFLOW_HOST: ${LANGFLOW_INTERNAL_HOST}
      LANGFLOW_PORT: ${LANGFLOW_INTERNAL_PORT}
      LANGFLOW_SUPERUSER: ${LANGFLOW_SUPERUSER}
      LANGFLOW_SUPERUSER_PASSWORD: ${LANGFLOW_SUPERUSER_PASSWORD}
      LANGFLOW_AUTO_LOGIN: "true"
      #LANGFLOW_CACHE_TYPE: redis
      LANGFLOW_CACHE_TYPE: memory
      LANGFLOW_REDIS_HOST: ${REDIS_HOST}
      LANGFLOW_REDIS_PORT: ${REDIS_PORT}
      LANGFLOW_REDIS_DB: 0
      LANGFLOW_REDIS_PASSWORD: ${REDIS_AUTH}
      LANGFLOW_REDIS_CACHE_EXPIRE: 3600
      LANGFLOW_COMPONENTS_PATH: /home
      LANGFLOW_WORKERS: 1 
      LANGFLOW_DEACTIVATE_TRACING: "false"
      LANGFLOW_DATABASE_URL: postgresql://langflow-user:langflow-pass@${POSTGRESQL_AI_SERVICE_HOST}:${POSTGRESQL_AI_INTERNAL_PORT}/langflow
      LANGFLOW_CONFIG_DIR: /app/langflow
      LANGFLOW_CORS_ORIGINS: '*'
      LANGFLOW_LOG_LEVEL: "${LANGFLOW_LOG_LEVEL}"
      LANGFLOW_ENABLE_LOG_RETRIEVAL: "true"
      LANGFLOW_PRETTY_LOGS: "true"
      LANGFLOW_LOG_RETRIEVER_BUFFER_SIZE: 10000
      LANGFLOW_CREATE_STARTER_PROJECTS: "true"
      LANGFLOW_UPDATE_STARTER_PROJECTS: "true"
      LANGFLOW_LAZY_LOAD_COMPONENTS: "false"
      LANGFUSE_SECRET_KEY: "${LANGFUSE_SECRET_KEY}"
      LANGFUSE_PUBLIC_KEY: "${LANGFUSE_PUBLIC_KEY}"
      LANGFUSE_HOST: "http://${LANGFUSE_WEB_SERVICE_HOST}:${LANGFUSE_WEB_INTERNAL_PORT}"
      LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT: "OLLAMA_URL"
      OLLAMA_URL: ${OLLAMA_SERVICE_HOST}:${OLLAMA_SERVICE_PORT}
    volumes:
      - ./langflow:/app/langflow:Z
      - ./langflow/flows:/home:Z
    depends_on:
      postgres-ai-server:
        condition: service_healthy
  docling-serve:
    hostname: ${DOCLING_SERVE_HOST}
    image: quay.io/docling-project/${DOCLING_SERVE_IMAGE}:${DOCLING_SERVE_VERSION}
    profiles:
      - docling
    deploy:
      resources:
        limits:
          cpus: ${DOCLING_SERVE_CPUS}
          memory: ${DOCLING_SERVE_MEM}
        reservations:
          memory: ${DOCLING_SERVE_MEM}
          devices:
            - driver: nvidia
              count: ${DOCLING_SERVE_GPUS} # alternatively, use `count: all` for all GPUs
              capabilities: [gpu]
    ports:
      - ${DOCLING_SERVE_SERVICE_PORT}:${DOCLING_SERVE_INTERNAL_PORT}
    environment:
      DOCLING_SERVE_ENABLE_UI: 1
      DOCLING_DEVICE: "cuda:0"
      DOCLING_NUM_THREADS: "12"
      DOCLING_SERVE_LAYOUT_BATCH_SIZE: "160"
      DOCLING_SERVE_TABLE_BATCH_SIZE:  "10"
      DOCLING_SERVE_OCR_BATCH_SIZE:    "20"
      DOCLING_PERF_PAGE_BATCH_SIZE:    "20"
      DOCLING_SERVE_OPTIONS_CACHE_SIZE: "1"
      DOCLING_SERVE_ENG_KIND: "local"
      DOCLING_SERVE_ENG_LOC_NUM_WORKERS: "1"
      DOCLING_SERVE_ENG_LOC_SHARE_MODELS: "true"
      DOCLING_SERVE_MAX_DOCUMENT_TIMEOUT: "604800"
      #DOCLING_SERVE_LOAD_MODELS_AT_BOOT: "true"
      ORT_DISABLE_AZURE: "1"
      RAPIDOCR_ORT_PROVIDERS: "CUDAExecutionProvider,TensorrtExecutionProvider,CPUExecutionProvider"
  comfyui:
    build:
      dockerfile: ./comfyui/Dockerfile
    hostname: ${COMFYUI_SERVICE_HOST}
    image: comfyui:${COMFYUI_VERSION}
    profiles:
      - comfyui
    deploy:
      resources:
        limits:
          cpus: ${COMFYUI_CPUS}
          memory: ${COMFYUI_MEM}
        reservations:
          memory: ${COMFYUI_MEM}
          devices:
            - driver: nvidia
              count: ${COMFYUI_GPUS}
              capabilities: [gpu]
    ports:
      - ${COMFYUI_SERVICE_PORT}:${COMFYUI_INTERNAL_PORT}
    volumes:
      # persist your models, output, and settings:
      - ./comfyui/comfyui-volume/models:/app/ComfyUI/models
      # persist your output images:
      - ./comfyui/comfyui-volume/output:/app/ComfyUI/output
      # persist all your settings & extra nodes:
      - ./comfyui/comfyui-volume/settings:/app/ComfyUI/user/default:rw
      # persist just your saved flows (overrides the workflows/ in default):
      - ./comfyui/comfyui-volume/flows:/app/ComfyUI/user/default/workflows:rw
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - LISTEN=${COMFYUI_LISTEN_ADDRESS}
    stdin_open: true
    tty: true
  anythingllm:
    hostname: ${ANYTHINGLLM_SERVICE_HOST}
    image: mintplexlabs/anythingllm:${ANYTHINGLLM_VERSION}
    profiles:
      - anythingllm
    deploy:
      resources:
        limits:
          cpus: ${ANYTHINGLLM_CPUS}
          memory: ${ANYTHINGLLM_MEM}
        reservations:
          memory: ${ANYTHINGLLM_MEM}
    ports:
      - ${ANYTHINGLLM_SERVICE_PORT}:${ANYTHINGLLM_INTERNAL_PORT}
    volumes:
      - ./anythingllm/storage:/app/server/storage:Z
      - ./anythingllm/.env:/app/server/.env:Z
    environment:
      STORAGE_DIR: /app/server/storage
  localai:
    hostname: ${LOCALAI_SERVICE_HOST}
    image: localai/localai:${LOCALAI_VERSION}
    profiles:
      - localai
    deploy:
      resources:
        limits:
          cpus: ${LOCALAI_CPUS}
          memory: ${LOCALAI_MEM}
        reservations:
          memory: ${LOCALAI_MEM}
    ports:
      - ${LOCALAI_SERVICE_PORT}:${LOCALAI_INTERNAL_PORT}
    volumes:
      - ./localai/models:/models:Z
    environment:
      LOCALAI_EXTERNAL_BACKENDS: "llama-cpp"
      PRELOAD_MODELS: '[{"url": "","name": "gpt4all-j"}]'
      #MODELS: "huggingface://ibm-granite/granite-4.0-h-micro"
      #MODELS: "https://huggingface.co/ibm-granite/granite-4.0-h-micro"
      #MODELS: "qwen3-4b,llama-3.2-3b-instruct:q8_0,ibm-granite_granite-4.0-h-tiny,ibm-granite_granite-4.0-h-micro"
      #MODELS: "huggingface://Qwen/Qwen3-4B,huggingface://ibm-granite/granite-4.0-h-tiny,huggingface://ibm-granite/granite-4.0-h-micro"
#    entrypoint: ["/bin/bash", "-c", "\
#      local-ai models install qwen3-4b && \
#      local-ai models install llama-3.2-3b-instruct:q8_0 && \
#      local-ai models install ibm-granite_granite-4.0-h-tiny && \
#      local-ai models install ibm-granite_granite-4.0-h-micro && \
#      wait"]
  ollama:
    hostname: ${OLLAMA_SERVICE_HOST}
    image: ollama/ollama:${OLLAMA_VERSION}
    profiles:
      - ollama
    deploy:
      resources:
        reservations:
          memory: ${OLLAMA_MEM}
          devices:
            - driver: nvidia
              count: ${OLLAMA_GPUS} # alternatively, use `count: all` for all GPUs
              capabilities: [gpu]
        limits:
          cpus: ${OLLAMA_CPUS}
          memory: ${OLLAMA_MEM}
    ports:
      - ${OLLAMA_SERVICE_PORT}:${OLLAMA_INTERNAL_PORT}
    volumes:
      - ./ollama:/root/.ollama:Z
    environment:
      OLLAMA_HOST: ${OLLAMA_INTERNAL_HOST}:${OLLAMA_INTERNAL_PORT}
      OLLAMA_ORIGINS: '*'
      OLLAMA_DEBUG: 1
      OLLAMA_KEEP_ALIVE: ${OLLAMA_KEEP_ALIVE}
      OLLAMA_MAX_LOADED_MODELS: ${OLLAMA_MAX_LOADED_MODELS}
      OLLAMA_FLASH_ATTENTION: ${OLLAMA_FLASH_ATTENTION}
      OLLAMA_KV_CACHE_TYPE: ${OLLAMA_KV_CACHE_TYPE}
    entrypoint: ["/bin/bash", "-c", "\
      ollama serve & \
      sleep 5 && \
      ollama pull ibm/granite4:micro-h && ollama pull qwen3:4b \
      && ollama pull ibm/granite4:tiny-h-f16 && ollama pull ibm/granite4:tiny-h-Q8_0 \
      && ollama pull qwen3:14b && ollama pull gpt-oss:20b && ollama pull hf.co/ibm-granite/granite-4.0-h-small-GGUF:Q4_K_M && ollama pull hf.co/ibm-granite/granite-4.0-h-small-GGUF:Q6_K \
      && ollama pull hf.co/lmstudio-community/DeepSeek-Coder-V2-Lite-Instruct-GGUF:Q6_K && ollama pull hf.co/second-state/StarCoder2-15B-GGUF:Q6_K \
      && ollama pull ibm/granite-embedding:278m&& ollama pull ibm/granite-docling:258m \
      && ollama pull llava:13b && ollama pull llama3.2-vision:11b \
      && wait"]
  vllm:
    hostname: ${VLLM_SERVICE_HOST}
    image: vllm/vllm-openai:${VLLM_VERSION}
    profiles:
      - vllm
    deploy:
      resources:
        reservations:
          memory: ${VLLM_MEM}
          devices:
            - driver: nvidia
              count: ${VLLM_GPUS} # alternatively, use `count: all` for all GPUs
              capabilities: [gpu]
        limits:
          cpus: ${VLLM_CPUS}
          memory: ${VLLM_MEM}
    ports:
      - ${VLLM_SERVICE_PORT}:${VLLM_INTERNAL_PORT}
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      HUGGING_FACE_HUB_TOKEN: ${VLLM_HUGGING_FACE_HUB_TOKEN}
      VLLM_HOST_IP: ${VLLM_INTERNAL_HOST}
    command: >
      --model ${VLLM_MODEL} --max_model_len=${VLLM_MAX_MODEL_LENGHT} --host 0.0.0.0 --cpu-offload-gb=${VLLM_CPU_OFFLOAD_GB} --gpu_memory_utilization=${VLLM_GPU_MEMORY_UTILIZATION}
      --max-num-seqs 2
  unsloth:
    hostname: ${UNSLOTH_SERVICE_HOST}
    image: unsloth/unsloth:${UNSLOTH_VERSION}
    profiles:
      - unsloth
    deploy:
      resources:
        limits:
          cpus: ${UNSLOTH_CPUS}
          memory: ${UNSLOTH_MEM}
        reservations:
          memory: ${UNSLOTH_MEM}
          devices:
            - driver: nvidia
              count: ${UNSLOTH_GPUS} # alternatively, use `count: all` for all GPUs
              capabilities: [gpu]
    volumes:
      - ./unsloth/workspace/work:/workspace/work:Z
      - ./unsloth/workspace/notebooks:/workspace/notebooks:Z
    ports:
      - ${UNSLOTH_SERVICE_PORT}:${UNSLOTH_INTERNAL_PORT}
      - ${UNSLOTH_SSH_SERVICE_PORT}:${UNSLOTH_SSH_INTERNAL_PORT}
    environment:
      JUPYTER_PASSWORD: "Password@1!"
      USER_PASSWORD: "Password@1!"
  open-webui:
    hostname: ${OPEN_WEBUI_SERVICE_HOST}
    image: ghcr.io/open-webui/open-webui:${OPEN_WEBUI_VERSION}
    profiles:
      - ollama
      - vllm
    deploy:
      resources:
        limits:
          cpus: ${OPEN_WEBUI_CPUS}
          memory: ${OPEN_WEBUI_MEM}
        reservations:
          memory: ${OPEN_WEBUI_MEM}
    volumes:
      - open-webui:/app/backend/data
    ports:
      - ${OPEN_WEBUI_SERVICE_PORT}:${OPEN_WEBUI_INTERNAL_PORT}
    environment:
      OLLAMA_BASE_URL: http://ollama:11430
      WEBUI_SECRET_KEY:
      ENABLE_LOGIN_FORM: true 
      ENABLE_SIGNUP: true
      VECTOR_DB: pgvector
      PGVECTOR_DB_URL: postgresql://openwebui-user:openwebui-pass@${POSTGRESQL_AI_SERVICE_HOST}:${POSTGRESQL_AI_INTERNAL_PORT}/openwebui
      DATABASE_URL: postgresql://openwebui-user:openwebui-pass@${POSTGRESQL_AI_SERVICE_HOST}:${POSTGRESQL_AI_INTERNAL_PORT}/openwebui
    depends_on:
      postgres-ai-server:
        condition: service_healthy
      ollama:
        condition: service_started
  mcp-context-forge:
    hostname: ${MCP_CONTEXT_FORGE_SERVICE_HOST}
    image: ghcr.io/ibm/mcp-context-forge:${MCP_CONTEXT_FORGE_VERSION}
    profiles:
      - core
    deploy:
      resources:
        limits:
          cpus: ${MCP_CONTEXT_FORGE_CPUS}
          memory: ${MCP_CONTEXT_FORGE_MEM}
        reservations:
          memory: ${MCP_CONTEXT_FORGE_MEM}
    ports:
      - ${MCP_CONTEXT_FORGE_SERVICE_PORT}:${MCP_CONTEXT_FORGE_INTERNAL_PORT}
    environment:
      MCPGATEWAY_UI_ENABLED: true
      MCPGATEWAY_ADMIN_API_ENABLED: true
      HOST: ${MCP_CONTEXT_FORGE_INTERNAL_HOST}
      PORT: ${MCP_CONTEXT_FORGE_INTERNAL_PORT}
      #APP_ROOT_PATH: ${MCP_CONTEXT_FORGE_ROOT_PATH}
      JWT_SECRET_KEY: my-test-key
      BASIC_AUTH_USER: admin
      BASIC_AUTH_PASSWORD: changeme
      AUTH_REQUIRED: true
      PLATFORM_ADMIN_EMAIL: admin@example.com
      PLATFORM_ADMIN_PASSWORD: changeme
      PLATFORM_ADMIN_FULL_NAME: "Platform Administrator"
      DATABASE_URL: postgresql://mcpgw-user:mcpgw-pass@${POSTGRESQL_AI_SERVICE_HOST}:${POSTGRESQL_AI_INTERNAL_PORT}/mcpgw
      SECURE_COOKIES: false
      PLUGINS_ENABLED: true
      # Enable internal observability
      OBSERVABILITY_ENABLED: true
      OBSERVABILITY_METRICS_ENABLED: true
      OBSERVABILITY_EVENTS_ENABLED: true
      # Automatically trace HTTP requests
      OBSERVABILITY_TRACE_HTTP_REQUESTS: true
      # Retention and limits
      OBSERVABILITY_TRACE_RETENTION_DAYS: 7
      OBSERVABILITY_MAX_TRACES: 100000
      # Trace sampling (1.0 = 100%, 0.1 = 10%)
      OBSERVABILITY_SAMPLE_RATE: 1.0
    depends_on:
      postgres-ai-server:
        condition: service_healthy
  bamoe-mcp-server:
    hostname: ${MCP_SERVER_SERVICE_HOST}
    image: quay.io/bamoe/mcp-server:${BAMOE_VERSION}
    #image: quay.io/bamoe/mcp-server:9.3.0-ibm-0007
    profiles:
      - core
    deploy:
      resources:
        limits:
          cpus: ${MCP_SERVER_CPUS}
          memory: ${MCP_SERVER_MEM}
        reservations:
          memory: ${MCP_SERVER_MEM}
    ports:
      - ${MCP_SERVER_SERVICE_PORT}:${MCP_SERVER_INTERNAL_PORT}
    environment:
      MCP_SERVER_OPENAPI_URLS: "http://${BPMN_APP_SERVICE_HOST}:${BPMN_APP_INTERNAL_PORT}/q/openapi,http://${DMN_APP_SERVICE_HOST}:${DMN_APP_INTERNAL_PORT}/q/openapi"
      MCP_SERVER_PORT: ${MCP_SERVER_INTERNAL_PORT}
      QUARKUS_HTTP_HOST: ${MCP_SERVER_INTERNAL_HOST}
      MCP_SERVER_SECURITY_ENABLED: false
      QUARKUS_OIDC_ENABLED: false
    depends_on:
      bamoe-bpmn-app:
        condition: service_healthy
      bamoe-dmn-app:
        condition: service_healthy
  bamoe-maven-repository:
    hostname: ${BAMOE_MAVEN_REPOSITORY_SERVICE_HOST}
    image: quay.io/bamoe/maven-repository:${BAMOE_VERSION}
    deploy:
      resources:
        limits:
          cpus: ${BAMOE_MAVEN_REPOSITORY_CPUS}
          memory: ${BAMOE_MAVEN_REPOSITORY_MEM}
        reservations:
          memory: ${BAMOE_MAVEN_REPOSITORY_MEM}
    profiles:
      - bamoe-maven-repository
    ports:
      - ${BAMOE_MAVEN_REPOSITORY_INTERNAL_PORT}:${BAMOE_MAVEN_REPOSITORY_SERVICE_PORT}
  bamoe-management-console:
    hostname: ${BAMOE_MANAGEMENT_CONSOLE_SERVICE_HOST}
    image: quay.io/bamoe/management-console:${BAMOE_VERSION}
    profiles:
      - core
    deploy:
      resources:
        limits:
          cpus: ${BAMOE_MANAGEMENT_CONSOLE_CPUS}
          memory: ${BAMOE_MANAGEMENT_CONSOLE_MEM}
        reservations:
          memory: ${BAMOE_MANAGEMENT_CONSOLE_MEM}
    ports:
      - ${BAMOE_MANAGEMENT_CONSOLE_SERVICE_PORT}:${BAMOE_MANAGEMENT_CONSOLE_INTERNAL_PORT}
    #depends_on:
    #  keycloak:
    #    condition: service_healthy
    environment:
      #RUNTIME_TOOLS_MANAGEMENT_CONSOLE_OIDC_CLIENT_CLIENT_ID: kie-management-console
      RUNTIME_TOOLS_MANAGEMENT_CONSOLE_BASE_PATH: bamoe-process-management-console
      #RUNTIME_TOOLS_MANAGEMENT_CONSOLE_USE_APACHE_HTTPD_BASE_PATH_ALIAS: "true"
      #RUNTIME_TOOLS_MANAGEMENT_CONSOLE_MANAGED_BUSINESS_SERVICES: '[{"name":"Kogito Secure App", "businessServiceUrl":"http://localhost:8081", "clientId":"kie-management-console"},{"name":"Kogito Unsecure App", "businessServiceUrl":"http://localhost:8082"}]'
  postgres-server:
    hostname: ${POSTGRESQL_SERVICE_HOST}
    image: 'postgres:${POSTGRESQL_VERSION}'
    profiles:
      - core
    deploy:
      resources:
        limits:
          cpus: ${POSTGRESQL_CPUS}
          memory: ${POSTGRESQL_MEM}
        reservations:
          memory: ${POSTGRESQL_MEM}
    ports:
      - ${POSTGRESQL_SERVICE_PORT}:${POSTGRESQL_INTERNAL_PORT}
    volumes:
      - './sql/postgres-server:/docker-entrypoint-initdb.d:Z'
      - './sql/postgres-server/postgresql.conf:/etc/postgresql.conf'
      - postgres-data:/var/lib/postgresql/data
    command:
      postgres -c config_file=/etc/postgresql.conf
    healthcheck:
      test:
        - CMD
        - pg_isready
        - '-q'
        - '-d'
        - kogito
        - '-U'
        - kogito-user
      timeout: 45s
      interval: 10s
      retries: 50
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
  postgres-ai-server:
    hostname: ${POSTGRESQL_AI_SERVICE_HOST}
    profiles:
      - core
      - ollama 
    build:
      context: pgvector
      dockerfile: Dockerfile
    deploy:
      resources:
        limits:
          cpus: ${POSTGRESQL_AI_CPUS}
          memory: ${POSTGRESQL_AI_MEM}
        reservations:
          memory: ${POSTGRESQL_AI_MEM}
    ports:
      - ${POSTGRESQL_AI_SERVICE_PORT}:${POSTGRESQL_AI_INTERNAL_PORT}
    volumes:
      - './sql/postgres-ai-server:/docker-entrypoint-initdb.d:Z'
      - './sql/postgres-ai-server/postgresql.conf:/etc/postgresql.conf'
      - postgres-ai-data:/var/lib/postgresql/data
    command:
      postgres -c config_file=/etc/postgresql.conf
    healthcheck:
      test:
        - CMD
        - pg_isready
        - '-q'
        - '-d'
        - postgres
        - '-U'
        - postgres
      timeout: 45s
      interval: 10s
      retries: 50
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
  pgadmin:
    image: dpage/pgadmin4:${PGADMIN_VERSION}
    hostname: ${PGADMIN_SERVICE_HOST}
    profiles:
      - core
      - ollama
    deploy:
      resources:
        limits:
          cpus: ${PGADMIN_CPUS}
          memory: ${PGADMIN_MEM}
        reservations:
          memory: ${PGADMIN_MEM}
    ports:
      - ${PGADMIN_SERVICE_PORT}:${PGADMIN_INTERNAL_PORT}
    volumes:
      - ./pgadmin/servers.json:/pgadmin4/servers.json:Z
      - ./pgadmin/pgpass:/pgadmin4/pgpass:Z
    entrypoint: >
      /bin/sh -c "
      cp -f /pgadmin4/pgpass /var/lib/pgadmin/;
      chmod 600 /var/lib/pgadmin/pgpass;
      /entrypoint.sh
      "
    environment:
      PGADMIN_DEFAULT_EMAIL: user@kogito.org
      PGADMIN_DEFAULT_PASSWORD: pass
      PGADMIN_CONFIG_SERVER_MODE: "False"
      PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED: "False"
      GUNICORN_ACCESS_LOGFILE: "/dev/null"
  keycloak:
    hostname: ${KEYCLOAK_SERVICE_HOST}
    image: quay.io/keycloak/keycloak:${KEYCLOAK_VERSION}
    profiles:
      - core
    deploy:
      resources:
        limits:
          cpus: ${KEYCLOAK_CPUS}
          memory: ${KEYCLOAK_MEM}
        reservations:
          memory: ${KEYCLOAK_MEM}
    ports:
      - ${KEYCLOAK_SERVICE_PORT}:${KEYCLOAK_INTERNAL_PORT}
    depends_on:
      postgres-server:
        condition: service_healthy
    volumes:
      - ./keycloak:/opt/keycloak/data/import:z
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "exec 3<>/dev/tcp/127.0.0.1/9000; echo -e 'GET /health/ready HTTP/1.1\r\nHost: localhost:9000\r\nConnection: close\r\n\r\n' >&3;cat <&3 | grep -q '\"status\": \"UP\"' && exit 0 || exit 1",
        ]
      interval: 2s
      timeout: 1s
      retries: 50
    environment:
      KC_HEALTH_ENABLED: "true"
      DB_VENDOR: POSTGRES
      DB_ADDR: ${POSTGRESQL_SERVICE_HOST}
      DB_DATABASE: keycloak
      DB_USER: kogito-user
      DB_SCHEMA: public
      DB_PASSWORD: kogito-pass
      KEYCLOAK_USER: admin
      KEYCLOAK_PASSWORD: admin
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
    command:
      start-dev --import-realm --hostname-port=${KEYCLOAK_SERVICE_PORT} --health-enabled=true
  redpanda:
    hostname: ${REDPANDA_SERVICE_HOST}
    image: redpandadata/redpanda:${REDPANDA_VERSION}
    profiles:
      - core
    deploy:
      resources:
        limits:
          cpus: ${REDPANDA_CPUS}
          memory: ${REDPANDA_MEM}
        reservations:
          memory: ${REDPANDA_MEM}
    ports:
      - 18081:18081
      - 18082:18082
      - 14001:8081
      - 14002:8082
      - 9092:9092
      - 28082:28082
      - 29092:2909
      - 9093:9093
      - 9644:9644
    volumes:
      - redpanda_data:/var/lib/redpanda/data
      #- ./redpanda/bootstrap.yml:/etc/redpanda/.bootstrap.yaml
    #environment:
      #DATA_TRANSFORMS_ENABLED: "true"
    command:
    - redpanda
    - start
    - --smp
    - '6'
    - --reserve-memory
    - 0M
    - --overprovisioned
    - --node-id
    - '0'
    - --kafka-addr
    - PLAINTEXT://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
    - --advertise-kafka-addr
    - PLAINTEXT://redpanda:9093,OUTSIDE://localhost:9092
    - --pandaproxy-addr
    - PLAINTEXT://0.0.0.0:28082,OUTSIDE://0.0.0.0:8082
    - --advertise-pandaproxy-addr
    - PLAINTEXT://redpanda:28082,OUTSIDE://localhost:8082
    - --schema-registry-addr internal://0.0.0.0:8081,external://0.0.0.0:18081
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8082/brokers" ]
      interval: 1s
      timeout: 1s
      retries: 50
  redpanda-console:
    hostname: ${REDPANDA_CONSOLE_SERVICE_HOST}
    image: redpandadata/console:${REDPANDA_CONSOLE_VERSION}
    profiles:
      - core
    deploy:
      resources:
        limits:
          cpus: ${REDPANDA_CONSOLE_CPUS}
          memory: ${REDPANDA_CONSOLE_MEM}
        reservations:
          memory: ${REDPANDA_CONSOLE_MEM}   
    # mount the local directory that contains your license key to the container.
    # give Redpanda Console read access to the license.
    volumes:
    # Remove ro?
      - ./license:/etc/redpanda:ro
    entrypoint: /bin/sh
    command: -c 'echo "$$CONSOLE_CONFIG_FILE" > /tmp/config.yml && echo "$$CONSOLE_ROLEBINDINGS_CONFIG_FILE" > /tmp/role-bindings.yml && /app/console'
    environment:
      #REDPANDA_LICENSE_FILEPATH: /etc/redpanda/redpanda.license
      CONFIG_FILEPATH: /tmp/config.yml
      CONSOLE_CONFIG_FILE: |
        kafka:
          brokers: ["redpanda:9093"]
        schemaRegistry:
          enabled: true
          urls: ["http://redpanda:8081"]
        redpanda:
          adminApi:
            enabled: true
            urls: ["http://redpanda:9644"]
        kafkaConnect:
          enabled: false
          clusters:
            - name: datagen
              url: http://redpanda-connect:8083
        authorization:
          roleBindings:
            - roleName: admin
              users:
                - loginType: basic
                  name: "admin"
    ports:
      - ${REDPANDA_CONSOLE_SERVICE_PORT}:${REDPANDA_CONSOLE_INTERNAL_PORT}
    depends_on:
      - redpanda
  wiremock-studio:
    hostname: ${WIREMOCK_SERVICE_HOST}
    image: jizuzquiza/wiremock-studio:${WIREMOCK_VERSION}
    profiles:
      - wiremock
    deploy:
      resources:
        limits:
          cpus: ${WIREMOCK_CPUS}
          memory: ${WIREMOCK_MEM}
        reservations:
          memory: ${WIREMOCK_MEM}
    volumes:
      - ./wiremock-data-storage:/home/wiremock:z
    ports:
    - ${WIREMOCK_SERVICE_PORT}:${WIREMOCK_INTERNAL_PORT}
  bamoe-bpmn-app:
    hostname: ${BPMN_APP_SERVICE_HOST}
    image: ${BPMN_APP_DOCKER_IMAGE_REGISTRY}/${BPMN_APP_DOCKER_IMAGE_GROUP}/bamoe-bpmn-app:${BPMN_APP_VERSION}
    profiles:
      - core
    deploy:
      resources:
        limits:
          cpus: ${BPMN_APP_CPUS}
          memory: ${BPMN_APP_MEM}
        reservations:
          memory: ${BPMN_APP_MEM}
    ports:
    - ${BPMN_APP_SERVICE_PORT}:${BPMN_APP_INTERNAL_PORT}
    environment:
      JAVA_OPTS: >
        -XX:ActiveProcessorCount=2
        -XX:MaxRAMPercentage=80.0
        -XX:+UseG1GC
        -XX:MinHeapFreeRatio=10
        -XX:MaxHeapFreeRatio=20
        -XX:GCTimeRatio=4
        -XX:AdaptiveSizePolicyWeight=90
        -XX:+ExitOnOutOfMemoryError
        -XX:+HeapDumpOnOutOfMemoryError
        -XX:HeapDumpPath=/home/jboss
        -Djava.util.logging.manager=org.jboss.logmanager.LogManager
        -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:8558
        -Dcom.sun.management.jmxremote
        -Dcom.sun.management.jmxremote.port=12346
        -Dcom.sun.management.jmxremote.rmi.port=12346
        -Dcom.sun.management.jmxremote.authenticate=false
        -Dcom.sun.management.jmxremote.ssl=false
        -Dcom.sun.management.jmxremote.host=0.0.0.0
        -Djava.rmi.server.hostname=localhost
      KOGITO_PERSISTENCE_PROTO_MARSHALLER: "false"
      QUARKUS_HTTP_CORS_ORIGINS: '*'
      QUARKUS_HTTP_HOST: ${BPMN_APP_INTERNAL_HOST}
      QUARKUS_HTTP_PORT: ${BPMN_APP_INTERNAL_PORT}
      GC_CONTAINER_OPTIONS: "-XX:+UseG1GC"
      # The number if IO threads used to perform IO. This will be automatically set to a reasonable value based on the number of CPU cores if it is not provided.
      # If this is set to a higher value than the number of Vert.x event loops then it will be capped at the number of event loops.
      # In general this should be controlled by setting quarkus.vertx.event-loops-pool-size, this setting should only be used if you want to limit the number
      # of HTTP io threads to a smaller number than the total number of IO threads.
      # QUARKUS_HTTP_IO_THREADS: 5
      # The number of event loops. By default, it matches the number of CPUs detected on the system.
      # QUARKUS_VERTX_EVENT_LOOPS_POOL_SIZE: 5
      # The maximum number of threads. If this is not specified then it will be automatically sized to the greatest of 8 * the number of available processors and 200.
      # For example if there are 4 processors the max threads will be 200. If there are 48 processors it will be 384.
      QUARKUS_THREAD_POOL_MAX_THREADS: 300
      # The core thread pool size. This number of threads will always be kept alive. Defaults to 1
      QUARKUS_THREAD_POOL_CORE_THREADS: 1
      # The amount of time a thread will stay alive with no work. Defaults to 30S
      QUARKUS_THREAD_POOL_KEEP_ALIVE_TIME: 30S
      # See https://quarkus.io/guides/datasource#jdbc-configuration
      QUARKUS_DATASOURCE_JDBC_URL: 'jdbc:postgresql://${POSTGRESQL_SERVICE_HOST}:${POSTGRESQL_SERVICE_PORT}/kogito'
      QUARKUS_DATASOURCE_USERNAME: kogito-user
      QUARKUS_DATASOURCE_PASSWORD: kogito-pass
      QUARKUS_DATASOURCE_DB_KIND: postgresql
      QUARKUS_DATASOURCE_JDBC_ENABLE_METRICS: "true"
      QUARKUS_DATASOURCE_JDBC_METRICS_ENABLED: "true"
      # XA transaction objects datasource
      QUARKUS_DATASOURCE__XOS__JDBC_URL: 'jdbc:postgresql://${POSTGRESQL_SERVICE_HOST}:${POSTGRESQL_SERVICE_PORT}/xa-object-store'
      QUARKUS_DATASOURCE__XOS__USERNAME: kogito-user
      QUARKUS_DATASOURCE__XOS__PASSWORD: kogito-pass
      QUARKUS_DATASOURCE__XOS__DB_KIND: postgresql
      QUARKUS.DATASOURCE__XOS__JDBC_TRANSACTIONS: disabled
      # The initial size of the pool. Usually you will want to set the initial size to match at least the minimal size, 
      # but this is not enforced so to allow for architectures which prefer a lazy initialization of the connections on boot, 
      # while being able to sustain a minimal pool size after boot. DEFAULT: null
      QUARKUS_DATASOURCE_JDBC_INITIAL_SIZE: 20
      # The datasource pool minimum size. DEFAULT: 0
      QUARKUS_DATASOURCE_JDBC_MIN_SIZE: 20
      # The datasource pool maximum size. DEFAULT: 20
      QUARKUS_DATASOURCE_JDBC_MAX_SIZE: 80
      # The interval at which we validate idle connections in the background. Set to 0 to disable background validation. DEFAULT: 2M
      # QUARKUS_DATASOURCE_JDBC_BACKGROUND_VALIDATION_INTERVAL: 2M
      # Perform foreground validation on connections that have been idle for longer than the specified interval. DEFAULT: null
      # QUARKUS_DATASOURCE_JDBC_FOREGROUND_VALIDATION_INTERVAL: 15S
      # The timeout before cancelling the acquisition of a new connection. DEFAULT: 5S
      # QUARKUS_DATASOURCE_JDBC_ACQUISITION_TIMEOUT: 5S
      # The interval at which we check for connection leaks. DEFAULT: disabled
      # QUARKUS_DATASOURCE_JDBC_LEAK_DETECTION_INTERVAL: 10M
      # The interval at which we try to remove idle connections. DEFAULT: 5M
      # QUARKUS_DATASOURCE_JDBC_IDLE_REMOVAL_INTERVAL: 5M
      # The max lifetime of a connection. DEFAULT: disabled
      # QUARKUS_DATASOURCE_JDBC_MAX_LIFETIME: 1M
      # The transaction isolation level. DEFAULT: null
      # QUARKUS_DATASOURCE_JDBC_TRANSACTION_ISOLATION_LEVEL: undefined, none, read-uncommitted, read-committed, repeatable-read, serializable
      QUARKUS_TRANSACTION-MANAGER_ENABLE-RECOVERY: "true"
      QUARKUS_TRANSACTION-MANAGER_OBJECT-STORE_DATASOURCE: "xos"
      QUARKUS_TRANSACTION-MANAGER_OBJECT-STORE_TYPE: "jdbc"
      QUARKUS_TRANSACTION-MANAGER_OBJECT-STORE_CREATE-TABLE: "true"
      KOGITO_SERVICE_URL: 'http://${BPMN_APP_SERVICE_HOST}:${BPMN_APP_SERVICE_PORT}'
      KOGITO_DATAINDEX_HTTP_URL: 'http://${BPMN_APP_SERVICE_HOST}:${BPMN_APP_SERVICE_PORT}'
      KOGITO_JOBS_SERVICE_URL: 'http://${BPMN_APP_SERVICE_HOST}:${BPMN_APP_SERVICE_PORT}'
      # The current chunk size in minutes the scheduler handles, it is used to keep a limit number of jobs scheduled
      # in the in-memory scheduler.
      #KOGITO_JOBS-SERVICE_SCHEDULERCHUNKINMINUTES: 10
      # The interval the job loading method runs to fetch the persisted jobs from the repository.
      #KOGITO_JOBS-SERVICE_LOADJOBINTERVALINMINUTES: 10
      # The interval based on the current time the job loading method uses to fetch jobs "FROM (now -
      # {@link #loadJobFromCurrentTimeIntervalInMinutes}) TO {@link #schedulerChunkInMinutes}"
      #KOGITO_JOBS-SERVICE_LOADJOBFROMCURRENTTIMEINTERVALINMINUTES: 0
      # Number of retries configured for the periodic jobs loading procedure. Every time the procedure is started this
      # value is considered.
      KOGITO_JOBS-SERVICE_LOADJOBRETRIES: 3
      # Error strategy to apply when the periodic jobs loading procedure has exceeded the jobLoadReties. NONE, FAIL_SERVICE
      #KOGITO_JOBS-SERVICE_LOADJOBERRORSTRATEGY: FAIL_SERVICE
      KOGITO_JOBS-SERVICE_FORCEEXECUTEEXPIREDJOBS: "true"
      KOGITO_JOBS-SERVICE_FORCEEXECUTEEXPIREDJOBSONSERVICESTART: "true"
      KOGITO_JOBS-SERVICE_NUMBEROFWORKERTHREADS: 10 
      KOGITO_JOBS-SERVICE_MAXNUMBEROFRETRIES: 12
      KOGITO_JOBS-SERVICE_RETRYMILLS: 3000
      KOGITO_JOBS-SERVICE_SCHEDULERCHUNKINMINUTES: 5
      KAFKA_BOOTSTRAP_SERVERS: redpanda:9093
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSINSTANCES-EVENTS_CONNECTOR: smallrye-kafka
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSINSTANCES-EVENTS_TOPIC: kogito-processinstances-events
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSINSTANCES-EVENTS_VALUE_SERIALIZER: org.apache.kafka.common.serialization.StringSerializer
      MP_MESSAGING_OUTGOING_KOGITO-USERTASKINSTANCES-EVENTS_CONNECTOR: smallrye-kafka
      MP_MESSAGING_OUTGOING_KOGITO-USERTASKINSTANCES-EVENTS_TOPIC: kogito-usertaskinstances-events
      MP_MESSAGING_OUTGOING_KOGITO-USERTASKINSTANCES-EVENTS_VALUE_SERIALIZER: org.apache.kafka.common.serialization.StringSerializer
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSDEFINITIONS-EVENTS_CONNECTOR: smallrye-kafka
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSDEFINITIONS-EVENTS_TOPIC: kogito-processdefinitions-events
      MP_MESSAGING_OUTGOING_KOGITO-PROCESSDEFINITIONS-EVENTS_VALUE_SERIALIZER: org.apache.kafka.common.serialization.StringSerializer
      KOGITO_EVENTS_GROUPING: "false"
      KIE_FLYWAY_ENABLED: "true"
      QUARKUS_REST_CLIENT__DUMMY_REST_SERVICE__URL: http://localhost:19002
      BAMOE_WORKFLOW_AI-AGENT-TASK_PROVIDER_LANGFLOW_BASE-URL: http://${LANGFLOW_SERVICE_HOST}:${LANGFLOW_INTERNAL_PORT}
      BAMOE_WORKFLOW_AI-AGENT-TASK_PROVIDER_LANGFLOW_API-KEY: ${LANGFLOW_API_KEY}
    depends_on:
      postgres-server:
        condition: service_healthy
      redpanda:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://${BPMN_APP_INTERNAL_HOST}:${BPMN_APP_INTERNAL_PORT}/q/health/ready"]
      interval: 5s
      retries: 5
  bamoe-bpmn-mcp-server:
    hostname: bamoe-bpmn-mcp-server
    image: jizuzquiza/bamoe-bpmn-mcp-server:1.0.0-SNAPSHOT
    profiles:
      - core
    ports:
      - 8889:8080
    environment:
      QUARKUS_REST_CLIENT__BAMOE_LOANS__URL: http://${BPMN_APP_SERVICE_HOST}:${BPMN_APP_INTERNAL_PORT}
    depends_on:
      bamoe-bpmn-app:
        condition: service_healthy
  bamoe-dmn-app:
    hostname: ${DMN_APP_SERVICE_HOST}
    image: ${DMN_APP_DOCKER_IMAGE_REGISTRY}/${DMN_APP_DOCKER_IMAGE_GROUP}/bamoe-dmn-app:${DMN_APP_VERSION}
    profiles:
      - core
    deploy:
      resources:
        limits:
          cpus: ${DMN_APP_CPUS}
          memory: ${DMN_APP_MEM}
        reservations:
          memory: ${DMN_APP_MEM}
    ports:
      - ${DMN_APP_SERVICE_PORT}:${DMN_APP_INTERNAL_PORT}
    environment:
      QUARKUS_HTTP_CORS_ORIGINS: /.*/
      GC_CONTAINER_OPTIONS: "-XX:+UseG1GC"
      QUARKUS_HTTP_HOST: ${DMN_APP_INTERNAL_HOST}
      QUARKUS_HTTP_PORT: ${DMN_APP_INTERNAL_PORT}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://${DMN_APP_INTERNAL_HOST}:${DMN_APP_INTERNAL_PORT}/q/health/ready"]
      interval: 5s
      retries: 5
volumes:
    redpanda_data: {}
    n8n-data: {}
    postgres-data: {}
    postgres-ai-data: {}
    redis: {}
    redisinsight: {}
    langfuse_clickhouse_data: {}
    langfuse_clickhouse_logs: {}
    langfuse_minio_data: {}
    open-webui: {}
