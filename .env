DOCKER_GATEWAY_HOST=localhost
DOCKER_INTERNAL_HOST=172.17.0.1
HOST_LOCAL_IP=192.168.1.220
MINIPC_HOST=ai.agentic.services.local

## nginx
NGINX_VERSION=1.28.0-alpine3.21
NGINX_INTERNAL_HOST=0.0.0.0
NGINX_INTERNAL_PORT=4000
NGINX_SERVICE_HOST=nginx
NGINX_SERVICE_PORT=4000
NGINX_CPUS=0.5
NGINX_MEM=64M

## Redis
REDIS_VERSION=8.4.0-alpine3.22
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_USER=default
REDIS_AUTH=myredissecret

## Redis Insight
REDIS_INSIGHT_VERSION=2.70.1
REDIS_INSIGHT_INTERNAL_HOST=redis-insight
REDIS_INSIGHT_INTERNAL_PORT=5540
REDIS_INSIGHT_SERVICE_HOST=redis-insight
REDIS_INSIGHT_SERVICE_PORT=5540
REDIS_INSIGHT_LISTEN_ADDRESS=0.0.0.0

## ClickHouse
CLICKHOUSE_VERSION=25.8

## n8n
N8N_VERSION=2.5.0
N8N_INTERNAL_HOST=localhost
N8N_INTERNAL_PORT=5678
N8N_SERVICE_HOST=n8n
N8N_SERVICE_PORT=5678
N8N_LISTEN_ADDRESS=0.0.0.0
N8N_INTERNET_URL=https://binocularly-glottic-darlene.ngrok-free.dev
N8N_EDITOR_BASE_URL=http://192.168.1.200:${N8N_INTERNAL_PORT}/n8n-editor/
N8N_PATH=/n8n-editor/
N8N_PUBLIC_API_ENDPOINT=/n8n-api
N8N_ENDPOINT_WEBHOOK=/n8n-webhook
N8N_ENDPOINT_WEBHOOK_TEST=/n8n-webhook-test
N8N_ENDPOINT_WEBHOOK_WAIT=/n8n-webhook-wait
N8N_CPUS=3
N8N_MEM=2500M

## langfuse
LANGFUSE_VERSION=3.153.0
LANGFUSE_SECRET_KEY=sk-lf-4e92b6f7-dedd-4b55-8d5e-8ec16d1b0111
LANGFUSE_PUBLIC_KEY=pk-lf-466b0e86-24e5-4c25-83c4-304c6ac53af3

## langfuse web
LANGFUSE_WEB_INTERNAL_HOST=0.0.0.0
LANGFUSE_WEB_INTERNAL_PORT=3000
LANGFUSE_WEB_SERVICE_HOST=langfuse-web
LANGFUSE_WEB_SERVICE_PORT=3000
LANGFUSE_WEB_SECRET_KEY=
LANGFUSE_WEB_PUBLIC_KEY=
LANGFUSE_WEB_CPUS=3
LANGFUSE_WEB_MEM=2500M
LANGFUSE_INIT_USER_EMAIL=jaime.izuzquiza@langfuse.com
LANGFUSE_INIT_USER_NAME=Jaime
LANGFUSE_INIT_USER_PASSWORD=Password@1!

## langfuse worker
LANGFUSE_WORKER_INTERNAL_HOST=0.0.0.0
LANGFUSE_WORKER_INTERNAL_PORT=3030
LANGFUSE_WORKER_SERVICE_HOST=langfuse-worker
LANGFUSE_WORKER_SERVICE_PORT=3030
LANGFUSE_WORKER_CPUS=3
LANGFUSE_WORKER_MEM=2500M

## MinIO
MINIO_VERSION=latest

## langflow
LANGFLOW_VERSION=1.7.3
LANGFLOW_NIGHTLY_VERSION=1.7.0.dev51
LANGFLOW_INTERNAL_HOST=0.0.0.0
LANGFLOW_INTERNAL_PORT=7860
LANGFLOW_SERVICE_HOST=langflow
LANGFLOW_SERVICE_PORT=7860
LANGFLOW_CPUS=3
LANGFLOW_MEM=4000M
LANGFLOW_LOG_LEVEL=INFO
LANGFLOW_SUPERUSER=admin
LANGFLOW_SUPERUSER_PASSWORD=Password@1!
LANGFLOW_API_KEY=sk-4ajoXwLAIL29-1NKYBXD_SCLEyL46oMHjJckS9EnA1k

## anythingllm
ANYTHINGLLM_VERSION=1.9.0
ANYTHINGLLM_INTERNAL_HOST=0.0.0.0
ANYTHINGLLM_INTERNAL_PORT=3001
ANYTHINGLLM_SERVICE_HOST=anythingllm
ANYTHINGLLM_SERVICE_PORT=3001
ANYTHINGLLM_CPUS=2
ANYTHINGLLM_MEM=1536M

## localai
LOCALAI_VERSION=v3.10.1-gpu-nvidia-cuda-13
LOCALAI_INTERNAL_HOST=0.0.0.0
LOCALAI_INTERNAL_PORT=8080
LOCALAI_SERVICE_HOST=localai
LOCALAI_SERVICE_PORT=3001
LOCALAI_CPUS=6
LOCALAI_MEM=10000M
LOCALAI_GPUS=all
LOCALAI_MAX_ACTIVE_BACKENDS=1
LOCALAI_WATCHDOG_IDLE=false
LOCALAI_WATCHDOG_IDLE_TIMEOUT=3m
LOCALAI_WATCHDOG_BUSY=false
LOCALAI_WATCHDOG_BUSY_TIMEOUT=10m
LOCALAI_DEBUG=true
#LOCALAI_MODELS=ollama://x/z-image-turbo:fp8,huggingface://ibm-granite/granite-4.0-h-tiny-GGUF:Q8_0
#LOCALAI_MODELS=sd-3.5-large-ggml,qwen-image-edit,flux.1-kontext-dev
LOCALAI_MODELS=Linaqruf/animagine-xl,huggingface://unsloth/FLUX.2-klein-4B-GGUF/flux-2-klein-4b-Q8_0.gguf,huggingface://unsloth/FLUX.2-klein-9B-GGUF/flux-2-klein-9b-Q8_0.gguf,sd-3.5-large-ggml,flux.1-kontext-dev,huggingface://unsloth/Qwen-Image-Edit-2511-GGUF/qwen-image-edit-2511-Q4_K_S.gguf,huggingface://nitrosocke/Ghibli-Diffusion
LOCALAI_EXTERNAL_BACKENDS=llama-cpp,stablediffusion-ggml,diffusers

## ollama
OLLAMA_VERSION=0.16.1
OLLAMA_INTERNAL_HOST=0.0.0.0
OLLAMA_INTERNAL_PORT=11434
OLLAMA_SERVICE_HOST=ollama
OLLAMA_SERVICE_PORT=11434
OLLAMA_CPUS=20
OLLAMA_MEM=65536M
OLLAMA_GPUS=all
OLLAMA_DEBUG=0
#OLLAMA_DEBUG=2
OLLAMA_KEEP_ALIVE=2m
OLLAMA_MAX_LOADED_MODELS=3
OLLAMA_FLASH_ATTENTION=1
#OLLAMA_KV_CACHE_TYPE=f16
OLLAMA_KV_CACHE_TYPE=q8_0
OLLAMA_CONTEXT_LENGTH=8192
OLLAMA_GPU_OVERHEAD=0
OLLAMA_PULL_COMMAND=
#    ollama pull ibm/granite4:micro-h 
#        && ollama pull ibm/granite4:micro-h
#        && ollama pull qwen3:4b
#        && ollama pull gpt-oss:20b
#        && ollama pull deepseek-r1:14b
#        && ollama pull qwen3:14b
#        && ollama pull granite-embedding:30m
#        ibm/granite4:small-h-q4_0

## vLLM
VLLM_VERSION=v0.15.1-x86_64-cu130
VLLM_INTERNAL_HOST=0.0.0.0
VLLM_INTERNAL_PORT=8000
VLLM_SERVICE_HOST=vllm
VLLM_SERVICE_PORT=8000
VLLM_CPUS=10
VLLM_MEM=75000M
VLLM_GPUS=all
VLLM_HUGGING_FACE_HUB_TOKEN=
#VLLM_MODEL=ibm-granite/granite-4.0-h-small
#VLLM_MODEL=lmstudio-community/granite-4.0-h-small-MLX-8bit
#VLLM_MODEL=ibm-granite/granite-4.0-h-tiny
#VLLM_MODEL=openai/gpt-oss-20b
VLLM_MODEL=google/gemma-3-12b-it-qat-int4-unquantized
VLLM_MAX_MODEL_LENGHT=4000
VLLM_MAX_NUM_SEQS=256
VLLM_CPU_OFFLOAD_GB=0
VLLM_GPU_MEMORY_UTILIZATION=0.95

## llama
LLAMA_VERSION=server-cuda13
LLAMA_INTERNAL_HOST=0.0.0.0
LLAMA_INTERNAL_PORT=8080
LLAMA_SERVICE_HOST=llama
LLAMA_SERVICE_PORT=11435
LLAMA_CPUS=20
LLAMA_MEM=48000M
LLAMA_GPUS=all
#LLAMA_MODEL=gpt-oss-20b-mxfp4.gguf
LLAMA_MODEL=granite-4.0-h-small-Q6_K.gguf
# max. number of layers to store in VRAM, either an exact number, 'auto', or 'all' (default: auto)
LLAMA_N_GPU_LAYERS=auto
# temperature (default: 0.80)
LLAMA_TEMPERATURE=0.1
# top-k sampling (default: 40, 0 = disabled)
LLAMA_TOP_K=40
# top-p sampling (default: 0.95, 1.0 = disabled)
LLAMA_TOP_P=0.95
# penalize repeat sequence of tokens (default: 1.00, 1.0 = disabled)
LLAMA_REPEAT_PENALTY=1.00
# repeat alpha frequency penalty (default: 0.00, 0.0 = disabled)
LLAMA_FREQUENCY_PENALTY=0.00
# repeat alpha presence penalty (default: 0.00, 0.0 = disabled)
LLAMA_PRESENCE_PENALTY=0.00
# size of the prompt context (default: 0, 0 = loaded from model)
LLAMA_CONTEXT_SIZE=32000
# number of tokens to predict (default: -1, -1 = infinity)
LLAMA_OUTPUT_TOKENS=16000
# controls the amount of thinking allowed; currently only one of: -1 for unrestricted thinking budget, or 0 to disable thinking (default: -1)
LLAMA_REASONING_BUDGET=0
# set Flash Attention use ('on', 'off', or 'auto', default: 'auto')
LLAMA_FLASH_ATTN=on
# whether to enable KV cache offloading (default: enabled)
LLAMA_KV_OFFLOAD=--kv-offload
#LLAMA_KV_OFFLOAD=--no-kv-offload
# KV cache data type for K allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1 (default: f16)
LLAMA_CACHE_TYPE_K=q8_0
# KV cache data type for V allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1 (default: f16)
LLAMA_CACHE_TYPE_V=q8_0
# logical maximum batch size (default: 2048)
LLAMA_BATCH_SIZE=2048
# physical maximum batch size (default: 512)
LLAMA_UBATCH_SIZE=512
LLAMA_CHAT_TEMPLATE=--chat-template-kwargs "{\"reasoning_effort\":\"medium\"}"

## Unsloth
UNSLOTH_VERSION=2026.1.2-pt2.9.0-cu12.8-update
UNSLOTH_INTERNAL_HOST=0.0.0.0
UNSLOTH_INTERNAL_PORT=8888
UNSLOTH_SERVICE_HOST=unsloth
UNSLOTH_SERVICE_PORT=8888
UNSLOTH_SSH_SERVICE_PORT=2222
UNSLOTH_SSH_INTERNAL_PORT=22
UNSLOTH_CPUS=6
UNSLOTH_MEM=20000M
UNSLOTH_GPUS=all

## Open-WebUI
OPEN_WEBUI_VERSION=0.8.1
OPEN_WEBUI_INTERNAL_HOST=0.0.0.0
OPEN_WEBUI_SERVICE_HOST=open-webui
OPEN_WEBUI_INTERNAL_PORT=8080
OPEN_WEBUI_SERVICE_PORT=8008
OPEN_WEBUI_CPUS=6
OPEN_WEBUI_MEM=8192M

## Comfyui
COMFYUI_VERSION=latest
COMFYUI_INTERNAL_HOST=0.0.0.0
COMFYUI_LISTEN_ADDRESS=0.0.0.0
COMFYUI_INTERNAL_PORT=8188
COMFYUI_SERVICE_HOST=comfyui
COMFYUI_SERVICE_PORT=8188
COMFYUI_CPUS=6
COMFYUI_MEM=8192M
COMFYUI_GPUS=all

## mcp-context-forge
MCP_CONTEXT_FORGE_VERSION=0.9.0
MCP_CONTEXT_FORGE_INTERNAL_HOST=0.0.0.0
MCP_CONTEXT_FORGE_INTERNAL_PORT=4444
MCP_CONTEXT_FORGE_SERVICE_HOST=mcp-gateway
MCP_CONTEXT_FORGE_SERVICE_PORT=4444
MCP_CONTEXT_FORGE_ROOT_PATH=/mcp-gateway
MCP_CONTEXT_FORGE_CPUS=2
MCP_CONTEXT_FORGE_MEM=1024M

## Docling serve
#DOCLING_SERVE_IMAGE=docling-serve-cpu
#DOCLING_SERVE_IMAGE=docling-serve
DOCLING_SERVE_IMAGE=docling-serve-cu128
DOCLING_SERVE_VERSION=v1.12.0
DOCLING_SERVE_HOST=docling-serve
DOCLING_SERVE_CPUS=10
DOCLING_SERVE_MEM=16384M
DOCLING_SERVE_SERVICE_PORT=5001
DOCLING_SERVE_INTERNAL_PORT=5001
DOCLING_SERVE_GPUS=all
DOCLING_SERVE_GRADIO_MCP_SERVER=True
DOCLING_NUM_THREADS=${DOCLING_SERVE_CPUS}
DOCLING_SERVE_LAYOUT_BATCH_SIZE=160
DOCLING_SERVE_TABLE_BATCH_SIZE=10
DOCLING_SERVE_OCR_BATCH_SIZE=20
DOCLING_PERF_PAGE_BATCH_SIZE=20
DOCLING_SERVE_OPTIONS_CACHE_SIZE=0
DOCLING_SERVE_ENG_KIND=local
#DOCLING_SERVE_ENG_KIND=rq
DOCLING_SERVE_ENG_LOC_NUM_WORKERS=1
DOCLING_SERVE_ENG_LOC_SHARE_MODELS=true
DOCLING_SERVE_LOAD_MODELS_AT_BOOT=false
DOCLING_SERVE_MAX_DOCUMENT_TIMEOUT=604800
DOCLING_SERVE_ENG_RQ_REDIS_URL=redis://${REDIS_USER}:${REDIS_AUTH}@${MINIPC_HOST}:${REDIS_PORT}/
DOCLING_SERVE_ENG_RQ_RESULTS_PREFIX=docling:results
DOCLING_SERVE_ENG_RQ_SUB_CHANNEL=docling:updates


## BAMOE version
#BAMOE_VERSION=9.3.0-ibm-0007
BAMOE_VERSION=9.3.1-ibm-0006

## bamoe-mcp-server
MCP_SERVER_INTERNAL_HOST=0.0.0.0
MCP_SERVER_INTERNAL_PORT=8888
MCP_SERVER_SERVICE_HOST=bamoe-mcp-server
MCP_SERVER_SERVICE_PORT=8888
MCP_SERVER_CPUS=0.5
MCP_SERVER_MEM=128M

## bamoe-maven-repository
BAMOE_MAVEN_REPOSITORY_INTERNAL_HOST=0.0.0.0
BAMOE_MAVEN_REPOSITORY_INTERNAL_PORT=8080
BAMOE_MAVEN_REPOSITORY_SERVICE_HOST=bamoe-maven-repository
BAMOE_MAVEN_REPOSITORY_SERVICE_PORT=9001
BAMOE_MAVEN_REPOSITORY_CPUS=0.5
BAMOE_MAVEN_REPOSITORY_MEM=128M

## bamoe-management-console
BAMOE_MANAGEMENT_CONSOLE_INTERNAL_HOST=0.0.0.0
BAMOE_MANAGEMENT_CONSOLE_INTERNAL_PORT=8080
BAMOE_MANAGEMENT_CONSOLE_SERVICE_HOST=bamoe-management-console
BAMOE_MANAGEMENT_CONSOLE_SERVICE_PORT=8280
BAMOE_MANAGEMENT_CONSOLE_CPUS=1
BAMOE_MANAGEMENT_CONSOLE_MEM=256M

## postgres-server
POSTGRESQL_VERSION=17.7
POSTGRESQL_INTERNAL_HOST=0.0.0.0
POSTGRESQL_INTERNAL_PORT=5432
POSTGRESQL_SERVICE_HOST=postgres-server
POSTGRESQL_SERVICE_PORT=5432
POSTGRESQL_CPUS=1
POSTGRESQL_MEM=368M

## postgres-ai-server
POSTGRESQL_AI_VERSION=17.7-pgvector
POSTGRESQL_AI_INTERNAL_HOST=0.0.0.0
POSTGRESQL_AI_INTERNAL_PORT=5432
POSTGRESQL_AI_SERVICE_HOST=postgres-ai-server
POSTGRESQL_AI_SERVICE_PORT=5433
POSTGRESQL_AI_CPUS=2
POSTGRESQL_AI_MEM=512M

## pgadmin
PGADMIN_VERSION=9.11.0
PGADMIN_INTERNAL_HOST=0.0.0.0
PGADMIN_INTERNAL_PORT=80
PGADMIN_SERVICE_HOST=pgadmin
PGADMIN_SERVICE_PORT=8055
PGADMIN_CPUS=1
PGADMIN_MEM=368M

## keycloak
KEYCLOAK_VERSION=26.2.1
KEYCLOAK_INTERNAL_HOST=0.0.0.0
KEYCLOAK_INTERNAL_PORT=8080
KEYCLOAK_SERVICE_HOST=keycloak
KEYCLOAK_SERVICE_PORT=8480
KEYCLOAK_CPUS=1
KEYCLOAK_MEM=368M

## redpanda
REDPANDA_VERSION=v25.1.9
REDPANDA_INTERNAL_HOST=0.0.0.0
REDPANDA_INTERNAL_PORT=8080
REDPANDA_SERVICE_HOST=redpanda
REDPANDA_SERVICE_PORT=8480
REDPANDA_CPUS=1
REDPANDA_MEM=1024M

## redpanda-console
REDPANDA_CONSOLE_VERSION=v3.1.3
REDPANDA_CONSOLE_INTERNAL_HOST=0.0.0.0
REDPANDA_CONSOLE_INTERNAL_PORT=8080
REDPANDA_CONSOLE_SERVICE_HOST=redpanda-console
REDPANDA_CONSOLE_SERVICE_PORT=9001
REDPANDA_CONSOLE_CPUS=1
REDPANDA_CONSOLE_MEM=512M

## wiremock-studio
WIREMOCK_VERSION=2.32.0-18
WIREMOCK_INTERNAL_HOST=0.0.0.0
WIREMOCK_INTERNAL_PORT=9000-9010
WIREMOCK_SERVICE_HOST=wiremock
WIREMOCK_SERVICE_PORT=19000-19010
WIREMOCK_CPUS=0.5
WIREMOCK_MEM=372M

## bamoe-bpmn-app
BPMN_APP_VERSION=1.0.0-SNAPSHOT
BPMN_APP_INTERNAL_HOST=0.0.0.0
BPMN_APP_INTERNAL_PORT=8881
BPMN_APP_SERVICE_HOST=bamoe-bpmn-app
BPMN_APP_SERVICE_PORT=8881
BPMN_APP_CPUS=2
BPMN_APP_MEM=768M
BPMN_APP_DOCKER_IMAGE_REGISTRY=docker.io
BPMN_APP_DOCKER_IMAGE_GROUP=jizuzquiza

## bamoe-dmn-app
DMN_APP_VERSION=1.0.0-SNAPSHOT
DMN_APP_INTERNAL_HOST=0.0.0.0
DMN_APP_INTERNAL_PORT=8882
DMN_APP_SERVICE_HOST=bamoe-dmn-app
DMN_APP_SERVICE_PORT=8882
DMN_APP_CPUS=1
DMN_APP_MEM=256M
DMN_APP_DOCKER_IMAGE_REGISTRY=docker.io
DMN_APP_DOCKER_IMAGE_GROUP=jizuzquiza
